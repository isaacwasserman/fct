{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24b7757a84e45eab3b546b2efb68dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/4983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 103, 256, 256]) torch.Size([1, 256, 256])\n",
      "torch.float16 torch.int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "permute() missing 1 required positional arguments: \"dims\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 440\u001b[0m\n\u001b[1;32m    438\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m get_dataset(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ds_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    439\u001b[0m vit \u001b[38;5;241m=\u001b[39m ViT(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m--> 440\u001b[0m \u001b[43mvit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 417\u001b[0m, in \u001b[0;36mViT.fit\u001b[0;34m(self, train_loader, val_loader, n_epochs)\u001b[0m\n\u001b[1;32m    415\u001b[0m scaler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mGradScaler()\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(val_loader, epoch)\n",
      "Cell \u001b[0;32mIn[1], line 400\u001b[0m, in \u001b[0;36mViT.train_epoch\u001b[0;34m(self, train_loader, optimizer, scaler, epoch)\u001b[0m\n\u001b[1;32m    398\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    399\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(imgs)\n\u001b[0;32m--> 400\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    402\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_accuracy(preds, labels)\n",
      "Cell \u001b[0;32mIn[1], line 344\u001b[0m, in \u001b[0;36mViT.calculate_loss\u001b[0;34m(self, y_hat, y)\u001b[0m\n\u001b[1;32m    342\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    343\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43my_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mprint\u001b[39m(y[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(y_hat, y)\n",
      "\u001b[0;31mTypeError\u001b[0m: permute() missing 1 required positional arguments: \"dims\""
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from datasets import load_dataset\n",
    "from utils import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATASET_PATH = \"data\"\n",
    "CHECKPOINT_PATH = \"checkpoints\"\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.mps.deterministic = True\n",
    "torch.backends.mps.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "num_workers = 0\n",
    "\n",
    "dataset = load_dataset(\"EduardoPacheco/FoodSeg103\")\n",
    "\n",
    "\n",
    "def dataset_transform(examples):\n",
    "    output_size = (256, 256)\n",
    "    crop_params = transforms.RandomResizedCrop.get_params(torch.ones(output_size), scale=(0.8, 1.0), ratio=(0.9, 1.1))\n",
    "    should_flip = torch.randint(0, 2, (1,)).item()\n",
    "    image_transform = transforms.Compose(\n",
    "        [\n",
    "            lambda x: torchvision.transforms.functional.crop(x, *crop_params),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomHorizontalFlip(should_flip),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "        ]\n",
    "    )\n",
    "    examples[\"image\"] = [image_transform(image) for image in examples[\"image\"]]\n",
    "    label_transform = transforms.Compose(\n",
    "        [\n",
    "            lambda x: torchvision.transforms.functional.crop(x, *crop_params),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomHorizontalFlip(should_flip),\n",
    "            transforms.ToTensor(),\n",
    "            lambda x: x.squeeze(0).long(),\n",
    "        ]\n",
    "    )\n",
    "    examples[\"label\"] = [label_transform(label) for label in examples[\"label\"]]\n",
    "    examples = {\"image\": examples[\"image\"], \"label\": examples[\"label\"]}\n",
    "    return examples\n",
    "\n",
    "\n",
    "dataset.set_transform(dataset_transform)\n",
    "\n",
    "\n",
    "def get_dataset(batch_size=64, ds_size=None):\n",
    "    train_set = dataset[\"train\"]\n",
    "    val_set = dataset[\"validation\"]\n",
    "    if ds_size is not None:\n",
    "        train_set = train_set.select(range(ds_size))\n",
    "        val_set = val_set.select(range(ds_size))\n",
    "    torch.random.manual_seed(42)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=256,\n",
    "        hidden_dim=512,\n",
    "        q_dim=512,\n",
    "        v_dim=256,\n",
    "        num_heads=8,\n",
    "        dropout=0.0,\n",
    "        internal_resolution=(32, 32),\n",
    "        block_index=0,\n",
    "        kernel_size=1,\n",
    "    ):\n",
    "        \"\"\"Attention Block.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of input and attention feature vectors\n",
    "            hidden_dim: Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "            dropout: Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm((embed_dim, *internal_resolution))\n",
    "        self.q_net = torch.nn.Conv2d(\n",
    "            embed_dim, q_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")\n",
    "        )\n",
    "        self.k_net = torch.nn.Conv2d(\n",
    "            embed_dim, q_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")\n",
    "        )\n",
    "        self.v_net = torch.nn.Conv2d(\n",
    "            embed_dim, v_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")\n",
    "        )\n",
    "        self.head_unification = torch.nn.Conv2d(\n",
    "            v_dim, embed_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")\n",
    "        )\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm((embed_dim, *internal_resolution))\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(embed_dim, hidden_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Conv2d(hidden_dim, embed_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "        self.num_heads = num_heads\n",
    "        self.block_index = block_index\n",
    "        self.q_dim = q_dim\n",
    "        self.v_dim = v_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def break_into_heads(self, M):\n",
    "        B, D, H, W = M.shape\n",
    "        h = self.num_heads\n",
    "        return M.reshape(B, h, D // h, H, W).flatten(0, 1)\n",
    "\n",
    "    def print_memory(self, prefix):\n",
    "        pass\n",
    "        # if self.block_index == 5:\n",
    "        #     print_memory(prefix)\n",
    "\n",
    "    def spatial_linear_self_attention(self, Q, K, V):\n",
    "        \"\"\"\n",
    "        Q: (B, Dq, H, W)\n",
    "        K: (B, Dq, H, W)\n",
    "        V: (B, Dv, H, W)\n",
    "        \"\"\"\n",
    "\n",
    "        B, Dq, H, W = Q.shape\n",
    "        _, Dv, _, _ = V.shape\n",
    "        h = self.num_heads\n",
    "\n",
    "        assert_shape(Q, (B, Dq, H, W))\n",
    "        assert_shape(K, (B, Dq, H, W))\n",
    "        assert_shape(V, (B, Dv, H, W))\n",
    "\n",
    "        self.print_memory(\"SA start\")\n",
    "\n",
    "        Q = torch.nn.functional.softmax(Q, dim=-3)\n",
    "        assert_shape(Q, (B, Dq, H, W))\n",
    "        self.print_memory(\"Q softmax\")\n",
    "\n",
    "        K = torch.nn.functional.softmax(K.flatten(-2), dim=-1).reshape(B, Dq, H, W)\n",
    "        assert_shape(K, (B, Dq, H, W))\n",
    "        self.print_memory(\"K softmax\")\n",
    "\n",
    "        Q = self.break_into_heads(Q)\n",
    "        assert_shape(Q, (B * h, Dq // h, H, W))\n",
    "        self.print_memory(\"Q into heads\")\n",
    "\n",
    "        K = self.break_into_heads(K)\n",
    "        assert_shape(K, (B * h, Dq // h, H, W))\n",
    "        self.print_memory(\"K into heads\")\n",
    "\n",
    "        V = self.break_into_heads(V)\n",
    "        assert_shape(V, (B * h, Dv // h, H, W))\n",
    "        self.print_memory(\"V into heads\")\n",
    "\n",
    "        KV = (K.unsqueeze(2) * V.unsqueeze(1)).sum(\n",
    "            dim=[-1, -2]\n",
    "        )  # (Bh, Dq // h, 1, H, W) x (Bh, 1, Dv // h, H, W) -> (Bh, Dq // h, Dv // h)\n",
    "        assert_shape(KV, (B * h, Dq // h, Dv // h))\n",
    "        self.print_memory(\"KV inner product\")\n",
    "\n",
    "        KV = KV.unsqueeze(-1).unsqueeze(-1).permute(0, 2, 1, 3, 4).flatten(0, 1)\n",
    "        assert_shape(KV, (B * h * (Dv // h), Dq // h, 1, 1))\n",
    "        self.print_memory(\"KV reshape\")\n",
    "\n",
    "        Q = Q.flatten(0, 1).unsqueeze(0)\n",
    "        assert_shape(Q, (1, B * h * (Dq // h), H, W))\n",
    "        self.print_memory(\"Q reshape\")\n",
    "\n",
    "        QKV = torch.nn.functional.conv2d(Q, KV, groups=B * h)\n",
    "        assert_shape(QKV, (1, B * h * Dv // h, H, W))\n",
    "        self.print_memory(\"QKV convolution\")\n",
    "\n",
    "        QKV = QKV.view(B, Dv, H, W)\n",
    "        assert_shape(QKV, (B, Dv, H, W))\n",
    "        self.print_memory(\"QKV reshape\")\n",
    "\n",
    "        QKV = self.head_unification(QKV)\n",
    "        self.print_memory(\"Head unification\")\n",
    "\n",
    "        return QKV\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, D, H, W = x.shape\n",
    "        Dq = self.q_dim\n",
    "        Dv = self.v_dim\n",
    "        after_norm_1 = self.layer_norm_1(x)\n",
    "        assert_shape(after_norm_1, (B, D, H, W))\n",
    "        Q = self.q_net(after_norm_1)\n",
    "        assert_shape(Q, (B, Dq, H, W))\n",
    "        K = self.k_net(after_norm_1)\n",
    "        assert_shape(K, (B, Dq, H, W))\n",
    "        V = self.v_net(after_norm_1)\n",
    "        assert_shape(V, (B, Dv, H, W))\n",
    "\n",
    "        attn = self.spatial_linear_self_attention(Q, K, V)\n",
    "        assert_shape(attn, (B, D, H, W))\n",
    "        x = x + attn\n",
    "        assert_shape(x, (B, D, H, W))\n",
    "        x = self.layer_norm_2(x)\n",
    "        assert_shape(x, (B, D, H, W))\n",
    "        x = x + self.feed_forward(x)\n",
    "        assert_shape(x, (B, D, H, W))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=256,\n",
    "        hidden_dim=512,\n",
    "        q_dim=512,\n",
    "        v_dim=256,\n",
    "        num_channels=3,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        num_classes=10,\n",
    "        dropout=0.0,\n",
    "        patch_equivalent_mode=True,\n",
    "        patch_width=4,\n",
    "        input_resolution=(32, 32),\n",
    "        transformer_kernel_size=1,\n",
    "    ):\n",
    "        \"\"\"Vision Transformer.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim: Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels: Number of channels of the input (3 for RGB)\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers: Number of layers to use in the Transformer\n",
    "            num_classes: Number of classes to predict\n",
    "            dropout: Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if patch_equivalent_mode:\n",
    "            embedding_kernel_size = patch_width\n",
    "            stride = patch_width\n",
    "            internal_resolution = (input_resolution[0] // patch_width, input_resolution[1] // patch_width)\n",
    "        else:\n",
    "            embedding_kernel_size = 1\n",
    "            stride = 1\n",
    "            internal_resolution = input_resolution\n",
    "        self.input_layer_cnn = torch.nn.Sequential(\n",
    "            torch.nn.ZeroPad2d(same_padding(embedding_kernel_size) if not patch_equivalent_mode else 0),\n",
    "            torch.nn.Conv2d(\n",
    "                num_channels, num_channels, kernel_size=embedding_kernel_size, stride=stride, padding=0, groups=num_channels\n",
    "            ),\n",
    "            torch.nn.Conv2d(num_channels, embed_dim, kernel_size=1, stride=1, padding=0),\n",
    "            torch.nn.GELU(),\n",
    "        )\n",
    "        self.transformer = torch.nn.Sequential(\n",
    "            *(\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    q_dim=q_dim,\n",
    "                    v_dim=v_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                    internal_resolution=internal_resolution,\n",
    "                    block_index=block_index,\n",
    "                    kernel_size=transformer_kernel_size,\n",
    "                )\n",
    "                for block_index in range(num_layers)\n",
    "            )\n",
    "        )\n",
    "        self.segmentation_head = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm((embed_dim, *input_resolution)),\n",
    "            torch.nn.ZeroPad2d(same_padding(transformer_kernel_size)),\n",
    "            torch.nn.Conv2d(embed_dim, num_classes, kernel_size=transformer_kernel_size),\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.positional_bias = torch.nn.Parameter(torch.randn(embed_dim, *internal_resolution))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply depthwise separable convolution embedding\n",
    "        x = self.input_layer_cnn(x)  # (B, D, H, W)\n",
    "        B, D, H, W = x.shape\n",
    "\n",
    "        # Add positional embedding\n",
    "        pos_embedding = self.positional_bias.unsqueeze(0).repeat(B, 1, 1, 1)  # (B, D, H, W)\n",
    "        x = x + pos_embedding\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Segmentation\n",
    "        out = self.segmentation_head(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "kill_defunct_processes()\n",
    "\n",
    "\n",
    "class ViT:\n",
    "    def __init__(self, **hyperparams):\n",
    "        super().__init__()\n",
    "        self.model = VisionTransformer(**model_kwargs).to(device)\n",
    "        prev_runs = [int(x.split(\"_\")[-1]) for x in os.listdir(\"runs\") if \"ViT_\" in x] + [-1]\n",
    "        self.run_id = f\"ViT_{max(prev_runs) + 1:03d}\"\n",
    "        self.log_dir = f\"runs/{self.run_id}\"\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "        self.best_accuracy = 0\n",
    "\n",
    "    def calculate_loss(self, y_hat, y):\n",
    "        print(y_hat.shape, y.shape)\n",
    "        print(y_hat.dtype, y.dtype)\n",
    "        plt.imshow(y_hat[0].argmax(dim=0).cpu().numpy())\n",
    "        plt.savefig(\"y_hat.png\")\n",
    "        plt.imshow(y[0].cpu().numpy())\n",
    "        plt.savefig(\"y.png\")\n",
    "        print(y_hat[0].permute().argmax(dim=0))\n",
    "        print(y[0])\n",
    "        return torch.nn.functional.cross_entropy(y_hat, y)\n",
    "\n",
    "    def calculate_accuracy(self, y_hat, y):\n",
    "        y_hat = y_hat.permute(0, 2, 3, 1).reshape(-1, y_hat.shape[1])\n",
    "        y = y.flatten()\n",
    "        return (y_hat.argmax(dim=1) == y).float().mean()\n",
    "\n",
    "    def checkpoint(self, accuracy):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            torch.save(self.model.state_dict(), f\"{self.log_dir}/vit.pth\")\n",
    "\n",
    "    def log_test(self, test_loader, epoch):\n",
    "        with torch.no_grad():\n",
    "            batch = next(iter(test_loader))\n",
    "            imgs, labels = batch\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            preds = self.model(imgs)\n",
    "            # Inverse normalization\n",
    "            imgs = imgs.cpu() * torch.tensor([0.24703223, 0.24348513, 0.26158784]).view(1, 3, 1, 1) + torch.tensor(\n",
    "                [0.49139968, 0.48215841, 0.44653091]\n",
    "            ).view(1, 3, 1, 1)\n",
    "            grid = create_image_grid(imgs, preds, labels, grid_size=(16, 16))\n",
    "            grid = torch.tensor(grid).permute(2, 0, 1)\n",
    "            self.writer.add_image(\"Test\", grid, epoch)\n",
    "\n",
    "    def validate(self, val_loader, epoch):\n",
    "        accumulated_loss = 0\n",
    "        accumulated_accuracy = 0\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                imgs, labels = batch\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                preds = self.model(imgs)\n",
    "                accumulated_loss += self.calculate_loss(preds, labels)\n",
    "                accumulated_accuracy += self.calculate_accuracy(preds, labels)\n",
    "        accumulated_loss /= len(val_loader)\n",
    "        accumulated_accuracy /= len(val_loader)\n",
    "\n",
    "        self.writer.add_scalar(\"Loss/val\", accumulated_loss, epoch)\n",
    "        self.writer.add_scalar(\"Accuracy/val\", accumulated_accuracy, epoch)\n",
    "\n",
    "        self.checkpoint(accumulated_accuracy)\n",
    "\n",
    "    def train_epoch(self, train_loader, optimizer, scaler, epoch):\n",
    "        steps_per_epoch = len(train_loader)\n",
    "        for batch_idx, batch in tqdm(enumerate(train_loader), desc=f\"Epoch {epoch+1}\", total=steps_per_epoch):\n",
    "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                imgs, labels = batch[\"image\"], batch[\"label\"]\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                preds = self.model(imgs)\n",
    "                loss = self.calculate_loss(preds, labels)\n",
    "                with torch.no_grad():\n",
    "                    accuracy = self.calculate_accuracy(preds, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            step = epoch * steps_per_epoch + batch_idx\n",
    "            self.writer.add_scalar(\"Loss/train\", loss, step)\n",
    "            self.writer.add_scalar(\"Accuracy/train\", accuracy, step)\n",
    "\n",
    "    def fit(self, train_loader, val_loader, n_epochs=1):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=3e-4, fused=True)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        for epoch in range(n_epochs):\n",
    "            self.train_epoch(train_loader, optimizer, scaler, epoch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.validate(val_loader, epoch)\n",
    "\n",
    "\n",
    "model_kwargs = {\n",
    "    \"embed_dim\": 256,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"q_dim\": 512,\n",
    "    \"v_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 6,\n",
    "    \"num_channels\": 3,\n",
    "    \"num_classes\": 103,\n",
    "    \"dropout\": 0.2,\n",
    "    \"patch_equivalent_mode\": False,\n",
    "    \"input_resolution\": (256, 256),\n",
    "    \"transformer_kernel_size\": 3,\n",
    "}\n",
    "\n",
    "train_loader, val_loader = get_dataset(batch_size=1, ds_size=None)\n",
    "vit = ViT(**model_kwargs)\n",
    "vit.fit(train_loader, val_loader, n_epochs=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2494403af0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGiCAYAAAC/NyLhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeoUlEQVR4nO3df2yV5f3/8VdL2yO/zulKaU+rgAWVH/JDBlgblbHR0AJjICwR7BwYApG1ZlBEVqMgblkdW7ZFhyNLFuoSQCURiWSS1WLLmKVKlSCgDSXMwugpCuk5UKS09Pr8sS/nu6MVKLQc3/T5SO6k576vc3rdV1qfnnPuU2Kcc04AABgRG+0JAADQEYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYErUwrVu3TrdfvvtuuWWW5SZman3338/WlMBABgSlXC99tprKiws1OrVq/Xhhx9qzJgxysnJ0cmTJ6MxHQCAITHR+CO7mZmZmjBhgv70pz9Jktra2jRgwAA98cQT+sUvfnGjpwMAMCTuRn/DCxcuqLq6WkVFReF9sbGxys7OVmVlZbv3aW5uVnNzc/h2W1ubTp8+rX79+ikmJqbL5wwA6FzOOZ05c0bp6emKje3Yi383PFxffPGFLl68qNTU1Ij9qamp+vTTT9u9T3FxsdasWXMjpgcAuIGOHTum2267rUP3ueHhuhZFRUUqLCwM3w4Ggxo4cKAe0DTFKT6KMwMAXItWtWi3/q6+fft2+L43PFzJycnq0aOHGhoaIvY3NDTI7/e3ex+PxyOPx/O1/XGKV1wM4QIAc/7f1RXX8nbPDb+qMCEhQePGjVNZWVl4X1tbm8rKypSVlXWjpwMAMCYqLxUWFhZq/vz5Gj9+vO6991798Y9/VFNTkx577LFoTAcAYEhUwvXwww/r888/16pVqxQIBHTPPfdox44dX7tgAwCAr4rK57iuVygUks/n0yTN5D0uADCo1bWoXNsUDAbl9Xo7dF/+ViEAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwJROD9dzzz2nmJiYiG3YsGHh4+fPn1d+fr769eunPn36aM6cOWpoaOjsaQAAblJd8ozr7rvvVn19fXjbvXt3+NiyZcv01ltvacuWLaqoqNCJEyc0e/bsrpgGAOAmFNclDxoXJ7/f/7X9wWBQf/3rX7Vp0yb94Ac/kCRt2LBBw4cP1549e3Tfffd1xXQAADeRLnnGdfjwYaWnp2vw4MHKy8tTXV2dJKm6ulotLS3Kzs4Ojx02bJgGDhyoysrKrpgKAOAm0+nPuDIzM1VSUqKhQ4eqvr5ea9as0YMPPqgDBw4oEAgoISFBiYmJEfdJTU1VIBD4xsdsbm5Wc3Nz+HYoFOrsaQMAjOj0cE2dOjX89ejRo5WZmalBgwbp9ddfV8+ePa/pMYuLi7VmzZrOmiIAwLAuvxw+MTFRd911l2pra+X3+3XhwgU1NjZGjGloaGj3PbFLioqKFAwGw9uxY8e6eNYAgG+rLg/X2bNndeTIEaWlpWncuHGKj49XWVlZ+HhNTY3q6uqUlZX1jY/h8Xjk9XojNgBA99TpLxU++eSTmjFjhgYNGqQTJ05o9erV6tGjh+bNmyefz6eFCxeqsLBQSUlJ8nq9euKJJ5SVlcUVhQCAq9Lp4Tp+/LjmzZunU6dOqX///nrggQe0Z88e9e/fX5L0hz/8QbGxsZozZ46am5uVk5Ojl19+ubOnAQC4ScU451y0J9FRoVBIPp9PkzRTcTHx0Z4OAKCDWl2LyrVNwWCww2//8LcKAQCmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmdDhcu3bt0owZM5Senq6YmBi9+eabEcedc1q1apXS0tLUs2dPZWdn6/DhwxFjTp8+rby8PHm9XiUmJmrhwoU6e/bsdZ0IAKB76HC4mpqaNGbMGK1bt67d42vXrtWLL76o9evXq6qqSr1791ZOTo7Onz8fHpOXl6eDBw+qtLRU27dv165du7R48eJrPwsAQLcR45xz13znmBht3bpVs2bNkvTfZ1vp6elavny5nnzySUlSMBhUamqqSkpKNHfuXH3yyScaMWKEPvjgA40fP16StGPHDk2bNk3Hjx9Xenr6Fb9vKBSSz+fTJM1UXEz8tU4fABAlra5F5dqmYDAor9fboft26ntcR48eVSAQUHZ2dnifz+dTZmamKisrJUmVlZVKTEwMR0uSsrOzFRsbq6qqqs6cDgDgJhTXmQ8WCAQkSampqRH7U1NTw8cCgYBSUlIiJxEXp6SkpPCYr2publZzc3P4digU6sxpAwAMMXFVYXFxsXw+X3gbMGBAtKcEAIiSTg2X3++XJDU0NETsb2hoCB/z+/06efJkxPHW1ladPn06POarioqKFAwGw9uxY8c6c9oAAEM6NVwZGRny+/0qKysL7wuFQqqqqlJWVpYkKSsrS42Njaqurg6P2blzp9ra2pSZmdnu43o8Hnm93ogNANA9dfg9rrNnz6q2tjZ8++jRo9q3b5+SkpI0cOBALV26VL/61a905513KiMjQ88++6zS09PDVx4OHz5cubm5WrRokdavX6+WlhYVFBRo7ty5V3VFIQCge+twuPbu3avvf//74duFhYWSpPnz56ukpERPPfWUmpqatHjxYjU2NuqBBx7Qjh07dMstt4Tvs3HjRhUUFGjy5MmKjY3VnDlz9OKLL3bC6QAAbnbX9TmuaOFzXABg27fmc1wAAHQ1wgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMKXD4dq1a5dmzJih9PR0xcTE6M0334w4vmDBAsXExERsubm5EWNOnz6tvLw8eb1eJSYmauHChTp79ux1nQgAoHvocLiampo0ZswYrVu37hvH5Obmqr6+Prxt3rw54nheXp4OHjyo0tJSbd++Xbt27dLixYs7PnsAQLcT19E7TJ06VVOnTr3sGI/HI7/f3+6xTz75RDt27NAHH3yg8ePHS5JeeuklTZs2Tb/73e+Unp7e0SkBALqRLnmPq7y8XCkpKRo6dKiWLFmiU6dOhY9VVlYqMTExHC1Jys7OVmxsrKqqqtp9vObmZoVCoYgNANA9dXq4cnNz9be//U1lZWX6zW9+o4qKCk2dOlUXL16UJAUCAaWkpETcJy4uTklJSQoEAu0+ZnFxsXw+X3gbMGBAZ08bAGBEh18qvJK5c+eGvx41apRGjx6tIUOGqLy8XJMnT76mxywqKlJhYWH4digUIl4A0E11+eXwgwcPVnJysmprayVJfr9fJ0+ejBjT2tqq06dPf+P7Yh6PR16vN2IDAHRPXR6u48eP69SpU0pLS5MkZWVlqbGxUdXV1eExO3fuVFtbmzIzM7t6OgAA4zr8UuHZs2fDz54k6ejRo9q3b5+SkpKUlJSkNWvWaM6cOfL7/Tpy5Iieeuop3XHHHcrJyZEkDR8+XLm5uVq0aJHWr1+vlpYWFRQUaO7cuVxRCAC4og4/49q7d6/Gjh2rsWPHSpIKCws1duxYrVq1Sj169ND+/fv1ox/9SHfddZcWLlyocePG6Z///Kc8Hk/4MTZu3Khhw4Zp8uTJmjZtmh544AH95S9/6byzAgDctGKccy7ak+ioUCgkn8+nSZqpuJj4aE8HANBBra5F5dqmYDDY4esW+FuFAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTCBcAwBTCBQAwhXABAEwhXAAAUwgXAMAUwgUAMIVwAQBMIVwAAFMIFwDAFMIFADCFcAEATCFcAABTOhSu4uJiTZgwQX379lVKSopmzZqlmpqaiDHnz59Xfn6++vXrpz59+mjOnDlqaGiIGFNXV6fp06erV69eSklJ0YoVK9Ta2nr9ZwMAuOl1KFwVFRXKz8/Xnj17VFpaqpaWFk2ZMkVNTU3hMcuWLdNbb72lLVu2qKKiQidOnNDs2bPDxy9evKjp06frwoULeu+99/TKK6+opKREq1at6ryzAgDctGKcc+5a7/z5558rJSVFFRUVmjhxooLBoPr3769Nmzbpxz/+sSTp008/1fDhw1VZWan77rtPb7/9tn74wx/qxIkTSk1NlSStX79eK1eu1Oeff66EhIQrft9QKCSfz6dJmqm4mPhrnT4AIEpaXYvKtU3BYFBer7dD972u97iCwaAkKSkpSZJUXV2tlpYWZWdnh8cMGzZMAwcOVGVlpSSpsrJSo0aNCkdLknJychQKhXTw4MF2v09zc7NCoVDEBgDonq45XG1tbVq6dKnuv/9+jRw5UpIUCASUkJCgxMTEiLGpqakKBALhMf8brUvHLx1rT3FxsXw+X3gbMGDAtU4bAGDcNYcrPz9fBw4c0KuvvtqZ82lXUVGRgsFgeDt27FiXf08AwLdT3LXcqaCgQNu3b9euXbt02223hff7/X5duHBBjY2NEc+6Ghoa5Pf7w2Pef//9iMe7dNXhpTFf5fF45PF4rmWqAICbTIeecTnnVFBQoK1bt2rnzp3KyMiIOD5u3DjFx8errKwsvK+mpkZ1dXXKysqSJGVlZenjjz/WyZMnw2NKS0vl9Xo1YsSI6zkXAEA30KFnXPn5+dq0aZO2bdumvn37ht+T8vl86tmzp3w+nxYuXKjCwkIlJSXJ6/XqiSeeUFZWlu677z5J0pQpUzRixAg9+uijWrt2rQKBgJ555hnl5+fzrAoAcEUduhw+Jiam3f0bNmzQggULJP33A8jLly/X5s2b1dzcrJycHL388ssRLwN+9tlnWrJkicrLy9W7d2/Nnz9fL7zwguLirq6jXA4PALZdz+Xw1/U5rmghXABgW9Q+xwUAwI1GuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJhCuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgCuECAJjSoXAVFxdrwoQJ6tu3r1JSUjRr1izV1NREjJk0aZJiYmIitscffzxiTF1dnaZPn65evXopJSVFK1asUGtr6/WfDQDgphfXkcEVFRXKz8/XhAkT1NraqqefflpTpkzRoUOH1Lt37/C4RYsW6fnnnw/f7tWrV/jrixcvavr06fL7/XrvvfdUX1+vn/70p4qPj9evf/3rTjglAMDNrEPh2rFjR8TtkpISpaSkqLq6WhMnTgzv79Wrl/x+f7uP8Y9//EOHDh3SO++8o9TUVN1zzz365S9/qZUrV+q5555TQkLCNZwGAKC7uK73uILBoCQpKSkpYv/GjRuVnJyskSNHqqioSOfOnQsfq6ys1KhRo5Samhrel5OTo1AopIMHD7b7fZqbmxUKhSI2AED31KFnXP+rra1NS5cu1f3336+RI0eG9z/yyCMaNGiQ0tPTtX//fq1cuVI1NTV64403JEmBQCAiWpLCtwOBQLvfq7i4WGvWrLnWqQIAbiLXHK78/HwdOHBAu3fvjti/ePHi8NejRo1SWlqaJk+erCNHjmjIkCHX9L2KiopUWFgYvh0KhTRgwIBrmzgAwLRreqmwoKBA27dv17vvvqvbbrvtsmMzMzMlSbW1tZIkv9+vhoaGiDGXbn/T+2Iej0derzdiAwB0Tx0Kl3NOBQUF2rp1q3bu3KmMjIwr3mffvn2SpLS0NElSVlaWPv74Y508eTI8prS0VF6vVyNGjOjIdAAA3VCHXirMz8/Xpk2btG3bNvXt2zf8npTP51PPnj115MgRbdq0SdOmTVO/fv20f/9+LVu2TBMnTtTo0aMlSVOmTNGIESP06KOPau3atQoEAnrmmWeUn58vj8fT+WcIALipxDjn3FUPjolpd/+GDRu0YMECHTt2TD/5yU904MABNTU1acCAAXrooYf0zDPPRLy899lnn2nJkiUqLy9X7969NX/+fL3wwguKi7u6joZCIfl8Pk3STMXFxF/t9AEA3xKtrkXl2qZgMNjht386FK5vC8IFALZdT7iu+arCaLrU2la1SOayCwBoVYuk///f844wGa4zZ85Iknbr71GeCQDgepw5c0Y+n69D9zH5UmFbW5tqamo0YsQIHTt2jMvj23Hps26sT/tYn8tjfa6MNbq8K62Pc05nzpxRenq6YmM79sksk8+4YmNjdeutt0oSn+u6Atbn8lify2N9row1urzLrU9Hn2ldwr/HBQAwhXABAEwxGy6Px6PVq1fzoeVvwPpcHutzeazPlbFGl9eV62Py4gwAQPdl9hkXAKB7IlwAAFMIFwDAFMIFADDFZLjWrVun22+/XbfccosyMzP1/vvvR3tKUfHcc88pJiYmYhs2bFj4+Pnz55Wfn69+/fqpT58+mjNnztf+Ec+bza5duzRjxgylp6crJiZGb775ZsRx55xWrVqltLQ09ezZU9nZ2Tp8+HDEmNOnTysvL09er1eJiYlauHChzp49ewPPoutcaX0WLFjwtZ+p3NzciDE36/oUFxdrwoQJ6tu3r1JSUjRr1izV1NREjLma36m6ujpNnz5dvXr1UkpKilasWKHW1tYbeSpd5mrWaNKkSV/7GXr88ccjxlzvGpkL12uvvabCwkKtXr1aH374ocaMGaOcnJyIf5iyO7n77rtVX18f3nbv3h0+tmzZMr311lvasmWLKioqdOLECc2ePTuKs+16TU1NGjNmjNatW9fu8bVr1+rFF1/U+vXrVVVVpd69eysnJ0fnz58Pj8nLy9PBgwdVWlqq7du3a9euXVq8ePGNOoUudaX1kaTc3NyIn6nNmzdHHL9Z16eiokL5+fnas2ePSktL1dLSoilTpqipqSk85kq/UxcvXtT06dN14cIFvffee3rllVdUUlKiVatWReOUOt3VrJEkLVq0KOJnaO3ateFjnbJGzph7773X5efnh29fvHjRpaenu+Li4ijOKjpWr17txowZ0+6xxsZGFx8f77Zs2RLe98knnzhJrrKy8gbNMLokua1bt4Zvt7W1Ob/f737729+G9zU2NjqPx+M2b97snHPu0KFDTpL74IMPwmPefvttFxMT4/7zn//csLnfCF9dH+ecmz9/vps5c+Y33qc7rc/JkyedJFdRUeGcu7rfqb///e8uNjbWBQKB8Jg///nPzuv1uubm5ht7AjfAV9fIOee+973vuZ///OffeJ/OWCNTz7guXLig6upqZWdnh/fFxsYqOztblZWVUZxZ9Bw+fFjp6ekaPHiw8vLyVFdXJ0mqrq5WS0tLxFoNGzZMAwcO7LZrdfToUQUCgYg18fl8yszMDK9JZWWlEhMTNX78+PCY7OxsxcbGqqqq6obPORrKy8uVkpKioUOHasmSJTp16lT4WHdan2AwKElKSkqSdHW/U5WVlRo1apRSU1PDY3JychQKhXTw4MEbOPsb46trdMnGjRuVnJyskSNHqqioSOfOnQsf64w1MvVHdr/44gtdvHgx4oQlKTU1VZ9++mmUZhU9mZmZKikp0dChQ1VfX681a9bowQcf1IEDBxQIBJSQkKDExMSI+6SmpioQCERnwlF26bzb+/m5dCwQCCglJSXieFxcnJKSkrrFuuXm5mr27NnKyMjQkSNH9PTTT2vq1KmqrKxUjx49us36tLW1aenSpbr//vs1cuRISbqq36lAINDuz9elYzeT9tZIkh555BENGjRI6enp2r9/v1auXKmamhq98cYbkjpnjUyFC5GmTp0a/nr06NHKzMzUoEGD9Prrr6tnz55RnBmsmjt3bvjrUaNGafTo0RoyZIjKy8s1efLkKM7sxsrPz9eBAwci3jNGpG9ao/99v3PUqFFKS0vT5MmTdeTIEQ0ZMqRTvreplwqTk5PVo0ePr13F09DQIL/fH6VZfXskJibqrrvuUm1trfx+vy5cuKDGxsaIMd15rS6d9+V+fvx+/9cu9GltbdXp06e75boNHjxYycnJqq2tldQ91qegoEDbt2/Xu+++q9tuuy28/2p+p/x+f7s/X5eO3Sy+aY3ak5mZKUkRP0PXu0amwpWQkKBx48aprKwsvK+trU1lZWXKysqK4sy+Hc6ePasjR44oLS1N48aNU3x8fMRa1dTUqK6urtuuVUZGhvx+f8SahEIhVVVVhdckKytLjY2Nqq6uDo/ZuXOn2trawr+A3cnx48d16tQppaWlSbq518c5p4KCAm3dulU7d+5URkZGxPGr+Z3KysrSxx9/HBH30tJSeb1ejRgx4sacSBe60hq1Z9++fZIU8TN03Wt0jReTRM2rr77qPB6PKykpcYcOHXKLFy92iYmJEVeodBfLly935eXl7ujRo+5f//qXy87OdsnJye7kyZPOOecef/xxN3DgQLdz5063d+9el5WV5bKysqI866515swZ99FHH7mPPvrISXK///3v3UcffeQ+++wz55xzL7zwgktMTHTbtm1z+/fvdzNnznQZGRnuyy+/DD9Gbm6uGzt2rKuqqnK7d+92d955p5s3b160TqlTXW59zpw545588klXWVnpjh496t555x333e9+1915553u/Pnz4ce4WddnyZIlzufzufLycldfXx/ezp07Fx5zpd+p1tZWN3LkSDdlyhS3b98+t2PHDte/f39XVFQUjVPqdFdao9raWvf888+7vXv3uqNHj7pt27a5wYMHu4kTJ4YfozPWyFy4nHPupZdecgMHDnQJCQnu3nvvdXv27In2lKLi4YcfdmlpaS4hIcHdeuut7uGHH3a1tbXh419++aX72c9+5r7zne+4Xr16uYceesjV19dHccZd791333WSvrbNnz/fOfffS+KfffZZl5qa6jwej5s8ebKrqamJeIxTp065efPmuT59+jiv1+see+wxd+bMmSicTee73PqcO3fOTZkyxfXv39/Fx8e7QYMGuUWLFn3tfwpv1vVpb10kuQ0bNoTHXM3v1L///W83depU17NnT5ecnOyWL1/uWlpabvDZdI0rrVFdXZ2bOHGiS0pKch6Px91xxx1uxYoVLhgMRjzO9a4R/6wJAMAUU+9xAQBAuAAAphAuAIAphAsAYArhAgCYQrgAAKYQLgCAKYQLAGAK4QIAmEK4AACmEC4AgCmECwBgyv8BeuOo1mVX2gkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(batch[\"label\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
