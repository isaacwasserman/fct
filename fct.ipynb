{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning matplotlib tensorboard einops line_profiler > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from utils import *\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "%autoreload 2\n",
    "\n",
    "DATASET_PATH = \"data\"\n",
    "CHECKPOINT_PATH = \"checkpoints\"\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.mps.deterministic = True\n",
    "torch.backends.mps.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "\n",
    "test_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "    ]\n",
    ")\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def get_dataset(batch_size=64, ds_len=45000):\n",
    "    # Loading the training dataset. We need to split it into a training and validation part\n",
    "    # We need to do a little trick because the validation set should not use the augmentation.\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "    val_dataset = torchvision.datasets.CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "    L.seed_everything(42)\n",
    "    train_set, _ = torch.utils.data.random_split(train_dataset, [ds_len, len(train_dataset) - ds_len])\n",
    "    L.seed_everything(42)\n",
    "    _, val_set = torch.utils.data.random_split(val_dataset, [ds_len, len(train_dataset) - ds_len])\n",
    "\n",
    "    # Loading the test set\n",
    "    test_set = torchvision.datasets.CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "    # We define a set of data loaders that we can use for various purposes later.\n",
    "    torch.random.manual_seed(42)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=256,\n",
    "        hidden_dim=512,\n",
    "        q_dim=512,\n",
    "        v_dim=256,\n",
    "        num_heads=8,\n",
    "        dropout=0.0,\n",
    "        internal_resolution=(32, 32),\n",
    "        block_index=0,\n",
    "        kernel_size=1,\n",
    "    ):\n",
    "        \"\"\"Attention Block.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of input and attention feature vectors\n",
    "            hidden_dim: Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "            dropout: Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm((embed_dim, *internal_resolution))\n",
    "        self.q_net = torch.nn.Conv2d(\n",
    "            embed_dim, q_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")\n",
    "        )\n",
    "        self.k_net = torch.nn.Conv2d(\n",
    "            embed_dim, q_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")\n",
    "        )\n",
    "        self.v_net = torch.nn.Conv2d(\n",
    "            embed_dim, v_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")\n",
    "        )\n",
    "        self.head_unification = torch.nn.Conv2d(\n",
    "            v_dim, embed_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")\n",
    "        )\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm((embed_dim, *internal_resolution))\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(embed_dim, hidden_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Conv2d(hidden_dim, embed_dim, kernel_size=kernel_size, padding=same_padding(kernel_size, format=\"single\")),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "        self.num_heads = num_heads\n",
    "        self.block_index = block_index\n",
    "        self.q_dim = q_dim\n",
    "        self.v_dim = v_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def break_into_heads(self, M):\n",
    "        B, D, H, W = M.shape\n",
    "        h = self.num_heads\n",
    "        return M.reshape(B, h, D // h, H, W).flatten(0, 1)\n",
    "\n",
    "    def print_memory(self, prefix):\n",
    "        pass\n",
    "\n",
    "    def spatial_linear_self_attention(self, Q, K, V):\n",
    "        \"\"\"\n",
    "        Q: (B, Dq, H, W)\n",
    "        K: (B, Dq, H, W)\n",
    "        V: (B, Dv, H, W)\n",
    "        \"\"\"\n",
    "\n",
    "        B, Dq, H, W = Q.shape\n",
    "        _, Dv, _, _ = V.shape\n",
    "        h = self.num_heads\n",
    "\n",
    "        assert_shape(Q, (B, Dq, H, W))\n",
    "        assert_shape(K, (B, Dq, H, W))\n",
    "        assert_shape(V, (B, Dv, H, W))\n",
    "\n",
    "        self.print_memory(\"SA start\")\n",
    "\n",
    "        Q = torch.nn.functional.softmax(Q, dim=-3)\n",
    "        assert_shape(Q, (B, Dq, H, W))\n",
    "        self.print_memory(\"Q softmax\")\n",
    "\n",
    "        K = torch.nn.functional.softmax(K.flatten(-2), dim=-1).reshape(B, Dq, H, W)\n",
    "        assert_shape(K, (B, Dq, H, W))\n",
    "        self.print_memory(\"K softmax\")\n",
    "\n",
    "        Q = self.break_into_heads(Q)\n",
    "        assert_shape(Q, (B * h, Dq // h, H, W))\n",
    "        self.print_memory(\"Q into heads\")\n",
    "\n",
    "        K = self.break_into_heads(K)\n",
    "        assert_shape(K, (B * h, Dq // h, H, W))\n",
    "        self.print_memory(\"K into heads\")\n",
    "\n",
    "        V = self.break_into_heads(V)\n",
    "        assert_shape(V, (B * h, Dv // h, H, W))\n",
    "        self.print_memory(\"V into heads\")\n",
    "\n",
    "        KV = (K.unsqueeze(2) * V.unsqueeze(1)).sum(\n",
    "            dim=[-1, -2]\n",
    "        )  # (Bh, Dq // h, 1, H, W) x (Bh, 1, Dv // h, H, W) -> (Bh, Dq // h, Dv // h)\n",
    "        assert_shape(KV, (B * h, Dq // h, Dv // h))\n",
    "        self.print_memory(\"KV inner product\")\n",
    "\n",
    "        KV = KV.unsqueeze(-1).unsqueeze(-1).permute(0, 2, 1, 3, 4).flatten(0, 1)\n",
    "        assert_shape(KV, (B * h * (Dv // h), Dq // h, 1, 1))\n",
    "        self.print_memory(\"KV reshape\")\n",
    "\n",
    "        Q = Q.flatten(0, 1).unsqueeze(0)\n",
    "        assert_shape(Q, (1, B * h * (Dq // h), H, W))\n",
    "        self.print_memory(\"Q reshape\")\n",
    "\n",
    "        QKV = torch.nn.functional.conv2d(Q, KV, groups=B * h)\n",
    "        assert_shape(QKV, (1, B * h * Dv // h, H, W))\n",
    "        self.print_memory(\"QKV convolution\")\n",
    "\n",
    "        QKV = QKV.view(B, Dv, H, W)\n",
    "        assert_shape(QKV, (B, Dv, H, W))\n",
    "        self.print_memory(\"QKV reshape\")\n",
    "\n",
    "        QKV = self.head_unification(QKV)\n",
    "        self.print_memory(\"Head unification\")\n",
    "\n",
    "        return QKV\n",
    "\n",
    "    def spatial_quadratic_self_attention(self, Q, K, V):\n",
    "        B, Dq, H, W = Q.shape\n",
    "        _, Dv, _, _ = V.shape\n",
    "        h = self.num_heads\n",
    "\n",
    "        assert_shape(Q, (B, Dq, H, W))\n",
    "        assert_shape(K, (B, Dq, H, W))\n",
    "        assert_shape(V, (B, Dv, H, W))\n",
    "\n",
    "        Q = self.break_into_heads(Q)\n",
    "        assert_shape(Q, (B * h, Dq // h, H, W))\n",
    "\n",
    "        K = self.break_into_heads(K)\n",
    "        assert_shape(K, (B * h, Dq // h, H, W))\n",
    "        self.print_memory(\"K into heads\")\n",
    "\n",
    "        V = self.break_into_heads(V)\n",
    "        assert_shape(V, (B * h, Dv // h, H, W))\n",
    "\n",
    "        Q_image = Q.flatten(0, 1).unsqueeze(0)\n",
    "        assert_shape(Q_image, (1, B * h * (Dq // h), H, W))\n",
    "\n",
    "        K_kernels = K.flatten(-2).flatten(0, 1).unsqueeze(-1).unsqueeze(-1)\n",
    "        assert_shape(K_kernels, (B * h * (Dq // h), H * W, 1, 1))\n",
    "        K_kernels = K_kernels.permute(1, 0, 2, 3)\n",
    "        assert_shape(K_kernels, (H * W, B * h * (Dq // h), 1, 1))\n",
    "        K_kernels = K_kernels.reshape(H * W, B * h, Dq // h, 1, 1).permute(1, 0, 2, 3, 4)\n",
    "        assert_shape(K_kernels, (B * h, H * W, Dq // h, 1, 1))\n",
    "        K_kernels = K_kernels.flatten(0, 1)\n",
    "        assert_shape(K_kernels, (B * h * H * W, Dq // h, 1, 1))\n",
    "\n",
    "        QK = torch.nn.functional.conv2d(Q_image, K_kernels, groups=B * h)\n",
    "        assert_shape(QK, (1, B * h * H * W, H, W))\n",
    "        QK = QK.squeeze(0).reshape(B * h, H * W, H, W).flatten(-2)\n",
    "        QK = torch.nn.functional.softmax(QK, dim=-1)\n",
    "        assert_shape(QK, (B * h, H * W, H * W))\n",
    "        QK = QK.reshape(B * h, H * W, H, W)\n",
    "        QKV = (QK.unsqueeze(2) * V.unsqueeze(1)).sum(\n",
    "            dim=[-1, -2]\n",
    "        )  # (Bh, HW, 1, H, W) x (Bh, 1, Dv//h, H, W) -> (Bh, HW, Dv // h, H, W) -> (Bh, HW, Dv // h)\n",
    "        assert_shape(QKV, (B * h, H * W, Dv // h))\n",
    "        QKV = QKV.permute(0, 2, 1).reshape(B * h, Dv // h, H, W)\n",
    "        assert_shape(QKV, (B * h, Dv // h, H, W))\n",
    "        QKV = QKV.reshape(B, Dv, H, W)\n",
    "        assert_shape(QKV, (B, Dv, H, W))\n",
    "\n",
    "        QKV = self.head_unification(QKV)\n",
    "        assert_shape(QKV, (B, self.embed_dim, H, W))\n",
    "\n",
    "        return QKV\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, D, H, W = x.shape\n",
    "        Dq = self.q_dim\n",
    "        Dv = self.v_dim\n",
    "        after_norm_1 = self.layer_norm_1(x)\n",
    "        assert_shape(after_norm_1, (B, D, H, W))\n",
    "        Q = self.q_net(after_norm_1)\n",
    "        assert_shape(Q, (B, Dq, H, W))\n",
    "        K = self.k_net(after_norm_1)\n",
    "        assert_shape(K, (B, Dq, H, W))\n",
    "        V = self.v_net(after_norm_1)\n",
    "        assert_shape(V, (B, Dv, H, W))\n",
    "\n",
    "        attn = self.spatial_linear_self_attention(Q, K, V)\n",
    "        assert_shape(attn, (B, D, H, W))\n",
    "        x = x + attn\n",
    "        assert_shape(x, (B, D, H, W))\n",
    "        x = self.layer_norm_2(x)\n",
    "        assert_shape(x, (B, D, H, W))\n",
    "        x = x + self.feed_forward(x)\n",
    "        assert_shape(x, (B, D, H, W))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=256,\n",
    "        hidden_dim=512,\n",
    "        q_dim=512,\n",
    "        v_dim=256,\n",
    "        num_channels=3,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        num_classes=10,\n",
    "        dropout=0.0,\n",
    "        patch_equivalent_mode=True,\n",
    "        patch_width=4,\n",
    "        input_resolution=(32, 32),\n",
    "        transformer_kernel_size=1,\n",
    "    ):\n",
    "        \"\"\"Vision Transformer.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim: Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels: Number of channels of the input (3 for RGB)\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers: Number of layers to use in the Transformer\n",
    "            num_classes: Number of classes to predict\n",
    "            dropout: Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if patch_equivalent_mode:\n",
    "            embedding_kernel_size = patch_width\n",
    "            stride = patch_width\n",
    "            internal_resolution = (input_resolution[0] // patch_width, input_resolution[1] // patch_width)\n",
    "        else:\n",
    "            embedding_kernel_size = 1\n",
    "            stride = 1\n",
    "            internal_resolution = input_resolution\n",
    "        self.input_layer_cnn = torch.nn.Sequential(\n",
    "            torch.nn.ZeroPad2d(same_padding(embedding_kernel_size) if not patch_equivalent_mode else 0),\n",
    "            torch.nn.Conv2d(\n",
    "                num_channels, num_channels, kernel_size=embedding_kernel_size, stride=stride, padding=0, groups=num_channels\n",
    "            ),\n",
    "            torch.nn.Conv2d(num_channels, embed_dim, kernel_size=1, stride=1, padding=0),\n",
    "            torch.nn.GELU(),\n",
    "        )\n",
    "        self.transformer = torch.nn.Sequential(\n",
    "            *(\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    q_dim=q_dim,\n",
    "                    v_dim=v_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                    internal_resolution=internal_resolution,\n",
    "                    block_index=block_index,\n",
    "                    kernel_size=transformer_kernel_size,\n",
    "                )\n",
    "                for block_index in range(num_layers)\n",
    "            )\n",
    "        )\n",
    "        self.classification_head = torch.nn.Sequential(torch.nn.LayerNorm(embed_dim), torch.nn.Linear(embed_dim, num_classes))\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.positional_bias = torch.nn.Parameter(torch.randn(embed_dim, *internal_resolution))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply depthwise separable convolution embedding\n",
    "        x = self.input_layer_cnn(x)  # (B, D, H, W)\n",
    "        B, D, H, W = x.shape\n",
    "\n",
    "        # Add positional embedding\n",
    "        pos_embedding = self.positional_bias.unsqueeze(0).repeat(B, 1, 1, 1)  # (B, D, H, W)\n",
    "        x = x + pos_embedding\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        pooled = x.reshape(B, D, -1).mean(dim=-1)  # (B, D, H, W) -> (B, D, HW) -> (B, D)\n",
    "\n",
    "        # Classification\n",
    "        out = self.classification_head(pooled)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 10 19:18:58 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:41:00.0 Off |                  N/A |\n",
      "| 45%   44C    P8             24W /  350W |       1MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0264ba8941a5438e9b7ec91ad58f3125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d60c19ab8340c2b1b6768797506d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation 1:   0%|          | 0/682 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous best accuracy: 0.00%, Current accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "kill_defunct_processes()\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class ViT():\n",
    "    def __init__(self, **hyperparams):\n",
    "        super().__init__()\n",
    "        self.model = VisionTransformer(**model_kwargs).to(device)\n",
    "        self.writer = SummaryWriter()\n",
    "        self.best_accuracy = 0\n",
    "\n",
    "    def calculate_loss(self, y_hat, y):\n",
    "        return torch.nn.functional.cross_entropy(y_hat, y)\n",
    "\n",
    "    def calculate_accuracy(self, y_hat, y):\n",
    "        return (y_hat.argmax(dim=-1) == y).float().mean()\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, n_epochs=1):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=3e-4, fused=True)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        steps_per_epoch = len(train_loader)\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_idx, batch in tqdm(enumerate(train_loader), desc=f\"Epoch {epoch+1}\", total=steps_per_epoch):\n",
    "                with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                    imgs, labels = batch\n",
    "                    imgs = imgs.to(device, non_blocking=True)\n",
    "                    labels = labels.to(device, non_blocking=True)\n",
    "                    preds = self.model(imgs)\n",
    "                    loss = self.calculate_loss(preds, labels)\n",
    "                    with torch.no_grad():\n",
    "                        accuracy = self.calculate_accuracy(preds, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                step = epoch * steps_per_epoch + batch_idx\n",
    "                self.writer.add_scalar(\"Loss/train\", loss, step)\n",
    "                self.writer.add_scalar(\"Accuracy/train\", accuracy, step)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                accumulated_loss = 0\n",
    "                accumulated_accuracy = 0\n",
    "                for batch in tqdm(val_loader, desc=f\"Validation {epoch+1}\"):\n",
    "                    with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                        imgs, labels = batch\n",
    "                        imgs = imgs.to(device, non_blocking=True)\n",
    "                        labels = labels.to(device, non_blocking=True)\n",
    "                        preds = self.model(imgs)\n",
    "                        accumulated_loss += self.calculate_loss(preds, labels)\n",
    "                        accuracy += self.calculate_accuracy(preds, labels)\n",
    "                accumulated_loss /= len(val_loader)\n",
    "                accumulated_accuracy /= len(val_loader)\n",
    "\n",
    "                self.writer.add_scalar(\"Loss/val\", accumulated_loss, epoch)\n",
    "                self.writer.add_scalar(\"Accuracy/val\", accumulated_accuracy, epoch)\n",
    "\n",
    "            print(f\"Previous best accuracy: {self.best_accuracy:.2f}%, Current accuracy: {accumulated_accuracy:.2f}%\")\n",
    "            if accumulated_accuracy > self.best_accuracy:\n",
    "                print(\"Saving model...\")\n",
    "                self.best_accuracy = accumulated_accuracy\n",
    "                torch.save(vit.state_dict(), \"vit.pth\")\n",
    "\n",
    "                \n",
    "            \n",
    "    \n",
    "\n",
    "standard_vit_equivalent_kwargs = {\n",
    "    \"embed_dim\": 256,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"q_dim\": 512,\n",
    "    \"v_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 6,\n",
    "    \"num_channels\": 3,\n",
    "    \"num_classes\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"patch_equivalent_mode\": True,\n",
    "    \"patch_width\": 4,\n",
    "    \"input_resolution\": (32, 32),\n",
    "    \"transformer_kernel_size\": 1\n",
    "}\n",
    "\n",
    "model_kwargs = standard_vit_equivalent_kwargs\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataset(batch_size=64, ds_len=6400)\n",
    "vit = ViT(**model_kwargs)\n",
    "vit.fit(train_loader, val_loader, n_epochs=1)\n",
    "\n",
    "\n",
    "\n",
    "# model, result = train_model(lr=3e-4, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, **model_kwargs)\n",
    "\n",
    "# vit = VisionTransformer(**model_kwargs).to(device)\n",
    "# optimizer = torch.optim.AdamW(vit.parameters(), lr=3e-4, fused=True)\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# n_epochs = 1\n",
    "\n",
    "# def train():\n",
    "#     for epoch in range(n_epochs):\n",
    "#         for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "#             with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "#                 imgs, labels = batch\n",
    "#                 imgs = imgs.to(device, non_blocking=True)\n",
    "#                 labels = labels.to(device, non_blocking=True)\n",
    "#                 preds = vit(imgs)\n",
    "#                 loss = torch.nn.functional.cross_entropy(preds, labels)\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size: 64\n",
    "# ViT (3.2m): 66 it/s\n",
    "# Standard ViT equivalent (4.4m): 36 it/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
