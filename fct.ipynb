{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning matplotlib tensorboard einops > /dev/null 2>&1\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from utils import *\n",
    "\n",
    "DATASET_PATH = \"data\"\n",
    "CHECKPOINT_PATH = \"checkpoints\"\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.mps.deterministic = True\n",
    "torch.backends.mps.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAADMCAYAAADjyBIdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABa7klEQVR4nO2deXxV9Zn/n7uvSW5WEgJJIAgIalFUVGoRrSAVtW60tqOAXeiMa+tobZ0OYjv6ctep1VrbSqtYRqxa7YYKWBdQQEGRRfYAISHrTe6+nPP9/eGL/HwWSKCZOjP3eb9evtrv4bnnfM93O9/c+3k+x2GMMaAoiqIoiqIUDM7PugKKoiiKoijKPxbdACqKoiiKohQYugFUFEVRFEUpMHQDqCiKoiiKUmDoBlBRFEVRFKXA0A2goiiKoihKgaEbQEVRFEVRlAJDN4CKoiiKoigFhm4AFUVRFEVRCgzdACqKMmAWLlwIDocDdu/e/VlXRWTNmjVwxhlnQCgUAofDAevXrz/iczQ0NMDMmTMHv3LKoLF7925wOBywcOHCz7oqivK/Ft0AKv8QDm4c1q5d+1lXRfk/Si6Xg8svvxy6urrgwQcfhKeeegrq6+vF2E2bNsHtt9/+P2Ij+8ILL8CMGTOgoqICvF4vDB06FGbNmgXLly/vi3n99dfB4XDAc88913fs4JyS/rv11lvRNR599FFwOBwwadKkQ9aDnqO4uBimTJkCf/rTn1hsPB6H+fPnw3nnnQdlZWX9bsY2b94M5513HoTDYSgrK4Mrr7wS2tvbj6CVBo+VK1fC7bffDtFo9DO5PuXRRx/VjazymeD+rCugKIoyGOzYsQOamprgiSeegG9+85uHjd20aRMsWLAAzjrrLGhoaPjHVJBgjIGrr74aFi5cCCeeeCJ873vfg+rqamhpaYEXXngBzjnnHHj77bfhjDPOOOx57rjjDhgxYgQ6dtxxx6HyokWLoKGhAVavXg3bt2+HUaNGiec699xz4aqrrgJjDDQ1NcFjjz0GF1xwAfzlL3+B6dOn98V1dHTAHXfcAXV1dfC5z30OXn/99UPWb9++ffCFL3wBSkpK4M4774R4PA733XcfbNiwAVavXg1er7eflhpcVq5cCQsWLIA5c+ZAJBL5h15b4tFHH4WKigqYM2fOZ10VpcDQDaCiKP8naGtrAwD4H/FQHwj3338/LFy4EG688UZ44IEHwOFw9P3bbbfdBk899RS43f0v0TNmzICTTz75kP++a9cuWLlyJTz//PMwb948WLRoEcyfP1+MHT16NPzTP/1TX/nSSy+FcePGwcMPP4w2gDU1NdDS0gLV1dWwdu1aOOWUUw55/TvvvBMSiQS89957UFdXBwAAp556Kpx77rmwcOFC+Pa3v93vPSqKMvjoT8DKZ8acOXMgHA7Dnj17YObMmRAOh6G2thZ+9rOfAQDAhg0b4Oyzz4ZQKAT19fXwzDPPoM93dXXBv/7rv8Lxxx8P4XAYiouLYcaMGfDBBx+wazU1NcGFF14IoVAIqqqq4Lvf/S4sXboUHA4H+/bi3XffhfPOOw9KSkogGAzClClT4O233x7QPWUyGZg/fz6MGjUKfD4fDB8+HG655RbIZDJ9MbNnzwa/3w+bN29Gn50+fTqUlpbC/v37j+j+Dv48+Oyzz8KCBQugtrYWioqK4LLLLoOenh7IZDJw4403QlVVFYTDYZg7dy6qD8AnP/9de+21sGjRIhgzZgz4/X6YOHEivPHGGwO677/85S9w5plnQigUgqKiIjj//PNh48aNKKa1tRXmzp0Lw4YNA5/PBzU1NXDRRRcN6GfY5cuX950/EonARRddhNpvzpw5MGXKFAAAuPzyy8HhcMBZZ50lnmvhwoVw+eWXAwDA1KlT+37ypOPgrbfeglNPPRX8fj+MHDkSfvvb37JzRaNRuPHGG2H48OHg8/lg1KhRcPfdd4Nt24e9n1QqBXfddReMHTsW7rvvPrT5O8iVV14Jp5566mHPMxAWLVoEpaWlcP7558Nll10GixYtGvBnjz32WKioqIAdO3ag4z6fD6qrqwd0jt///vcwc+bMvs0fAMAXv/hFGD16NDz77LP9fj4ajcKcOXOgpKQEIpEIzJ49W/z59sMPP4Q5c+bAyJEjwe/3Q3V1NVx99dXQ2dnZF3P77bfDzTffDAAAI0aM6Ov7g2PwySefhLPPPhuqqqrA5/PBuHHj4LHHHmPXWrt2LUyfPh0qKiogEAjAiBEj4Oqrr0Yxtm3DQw89BOPHjwe/3w9DhgyBefPmQXd3d19MQ0MDbNy4Ef72t7/11eVQ41ZRBhv9BlD5TLEsC2bMmAFf+MIX4J577oFFixbBtddeC6FQCG677Tb4+te/Dpdccgn8/Oc/h6uuugpOP/30vp+7du7cCS+++CJcfvnlMGLECDhw4AA8/vjjMGXKFNi0aRMMHToUAAASiQScffbZ0NLSAjfccANUV1fDM888AytWrGD1Wb58OcyYMQMmTpwI8+fPB6fT2fdQePPNNw/7QLZtGy688EJ466234Nvf/jYce+yxsGHDBnjwwQdh69at8OKLLwIAwMMPPwzLly+H2bNnw6pVq8DlcsHjjz8Or7zyCjz11FN99R7o/R3krrvugkAgALfeeits374dfvrTn4LH4wGn0wnd3d1w++23wzvvvAMLFy6EESNGwL//+7+jz//tb3+D//qv/4Lrr78efD4fPProo3DeeefB6tWr2U+Kn+app56C2bNnw/Tp0+Huu++GZDIJjz32GHz+85+HdevW9f3Eeumll8LGjRvhuuuug4aGBmhra4NXX30V9uzZc9ifYV977TWYMWMGjBw5Em6//XZIpVLw05/+FCZPngzvv/8+NDQ0wLx586C2thbuvPNOuP766+GUU06BIUOGiOf7whe+ANdffz3853/+J/zwhz+EY489FgCg738BALZv3w6XXXYZfOMb34DZs2fDr3/9a5gzZw5MnDgRxo8fDwAAyWQSpkyZAs3NzTBv3jyoq6uDlStXwg9+8ANoaWmBhx566JD39NZbb0FXVxfceOON4HK5Dhk3EHp6eqCjowMdq6io6Pv/ixYtgksuuQS8Xi9cccUV8Nhjj8GaNWsO+63dp8/d3d0NjY2NR1W35uZmaGtrE7+hPPXUU+HPf/7zYT9vjIGLLroI3nrrLfjOd74Dxx57LLzwwgswe/ZsFvvqq6/Czp07Ye7cuVBdXQ0bN26EX/ziF7Bx40Z45513wOFwwCWXXAJbt26F3/3ud/Dggw/2tVNlZSUAADz22GMwfvx4uPDCC8HtdsPLL78M//Iv/wK2bcM111wDAJ980zxt2jSorKyEW2+9FSKRCOzevRuef/55VJ958+bBwoULYe7cuXD99dfDrl274JFHHoF169bB22+/DR6PBx566CG47rrrIBwOw2233QYAcMhxqyiDjlGUfwBPPvmkAQCzZs2avmOzZ882AGDuvPPOvmPd3d0mEAgYh8NhFi9e3Hd8y5YtBgDM/Pnz+46l02ljWRa6zq5du4zP5zN33HFH37H777/fAIB58cUX+46lUikzduxYAwBmxYoVxhhjbNs2xxxzjJk+fbqxbbsvNplMmhEjRphzzz33sPf41FNPGafTad588010/Oc//7kBAPP222/3HVu6dKkBAPOTn/zE7Ny504TDYfPlL38ZfW6g97dixQoDAOa4444z2Wy27/gVV1xhHA6HmTFjBjrH6aefburr69ExADAAYNauXdt3rKmpyfj9fnPxxRf3HTvYj7t27TLGGBOLxUwkEjHf+ta30PlaW1tNSUlJ3/Hu7m4DAObee+8V2+5wTJgwwVRVVZnOzs6+Yx988IFxOp3mqquuYu2wZMmSfs+5ZMkS1Pefpr6+3gCAeeONN/qOtbW1GZ/PZ2666aa+Yz/+8Y9NKBQyW7duRZ+/9dZbjcvlMnv27Dnk9R9++GEDAOaFF17ot67GyPd2sC+k/w6ydu1aAwDm1VdfNcZ8MsaHDRtmbrjhBnYNADDf+MY3THt7u2lrazNr16415513Xr/9tmbNGgMA5sknnzzkv/32t79l/3bzzTcbADDpdPqQ537xxRcNAJh77rmn71g+nzdnnnkmu2YymWSf/93vfsf68t5770Vj+NNI55g+fboZOXJkX/mFF15gaxnlzTffNABgFi1ahI7/9a9/ZcfHjx9vpkyZcshzKcp/F/oTsPKZ82nBfiQSgTFjxkAoFIJZs2b1HR8zZgxEIhHYuXNn3zGfzwdO5ydD2LIs6OzshHA4DGPGjIH333+/L+6vf/0r1NbWwoUXXth3zO/3w7e+9S1Uj/Xr18O2bdvga1/7GnR2dkJHRwd0dHRAIpGAc845B954443D/rS3ZMkSOPbYY2Hs2LF9n+3o6ICzzz4bAAB94zht2jSYN28e3HHHHXDJJZeA3++Hxx9/HJ1voPd3kKuuugo8Hk9fedKkSX2JBp9m0qRJsHfvXsjn8+j46aefDhMnTuwr19XVwUUXXQRLly4Fy7LEe3711VchGo3CFVdcge7Z5XLBpEmT+u45EAiA1+uF119/Hf0E1h8tLS2wfv16mDNnDpSVlfUdP+GEE+Dcc8/t9xuko2XcuHFw5pln9pUrKythzJgxaPwtWbIEzjzzTCgtLUX3/sUvfhEsyzrsz+e9vb0AAFBUVPR31/VnP/sZvPrqq+i/gyxatAiGDBkCU6dOBYBPfur/yle+AosXLxb79Fe/+hVUVlZCVVUVnHzyybBs2TK45ZZb4Hvf+95R1S2VSgHAJ2OZ4vf7UYzEn//8Z3C73fDP//zPfcdcLhdcd911LDYQCPT9/3Q6DR0dHXDaaacBAIjzReLT5zj4zeqUKVNg586d0NPTAwD/X2P6xz/+EXK5nHieJUuWQElJCZx77rlobEycOBHC4bD464Oi/KPRn4CVzxS/39/388tBSkpKYNiwYUwXVVJSgjYPtm3Dww8/DI8++ijs2rULPdDKy8v7/n9TUxM0Njay89FMyG3btgEAiD8vHaSnpwdKS0vFf9u2bRts3ryZ3c9BDiYpHOS+++6DP/zhD7B+/Xp45plnoKqqCv37QO/vIJ/WWAF80l4AAMOHD2fHbduGnp4edJ5jjjmGnXP06NGQTCahvb1d1HwdbLODm1xKcXExAHyyAbj77rvhpptugiFDhsBpp50GM2fOhKuuuuqwWrKmpiYA+OQPAMqxxx4LS5cuhUQiAaFQ6JDnOBpoWwIAlJaWovG3bds2+PDDDwfc35/mYLvEYrG/s6af/JQq/cRqWRYsXrwYpk6dCrt27eo7PmnSJLj//vth2bJlMG3aNPSZiy66CK699lrIZrOwZs0auPPOOyGZTPb9IXKkHNxQUc0pwCebtE/HSDQ1NUFNTQ2Ew2F0XBoPXV1dsGDBAli8eDFr+4Obt/54++23Yf78+bBq1SpIJpPsHCUlJTBlyhS49NJLYcGCBfDggw/CWWedBV/+8pfha1/7Wt9Gd9u2bdDT08Pm9EEONzYU5R+FbgCVz5RD6Z8OddwY0/f/77zzTvjRj34EV199Nfz4xz+GsrIycDqdcOONN/Yrwpc4+Jl7770XJkyYIMbQBxH9/PHHHw8PPPCA+O90I7Zu3bq+B8GGDRvgiiuuQP9+pPf397Tl0XKwHk899ZS4kft0FuuNN94IF1xwAbz44ouwdOlS+NGPfgR33XUXLF++HE488cS/uy6DyUDazLZtOPfcc+GWW24RY0ePHn3I848dOxYAPun3L3/5y0df0cOwfPlyaGlpgcWLF8PixYvZvy9atIhtAIcNGwZf/OIXAQDgS1/6ElRUVMC1114LU6dOhUsuueSI61BTUwMAn3yTS2lpaYGysjLx28GjYdasWbBy5Uq4+eabYcKECRAOh8G2bTjvvPMGtB7s2LEDzjnnHBg7diw88MADMHz4cPB6vfDnP/8ZHnzwwb5zHPRjfOedd+Dll1+GpUuXwtVXXw33338/vPPOO33XraqqOmTCzaH+aFCUfyS6AVT+1/Lcc8/B1KlT4Ve/+hU6Ho1GkQi+vr4eNm3aBMYY9C3g9u3b0ecOCt2Li4v7HoJHQmNjI3zwwQdwzjnniFmdnyaRSMDcuXNh3LhxcMYZZ8A999wDF198MRLmD/T+BouD3+Z9mq1bt0IwGDzkA+tgm1VVVQ2ozRobG+Gmm26Cm266CbZt2wYTJkyA+++/H55++mkx/qCR88cff8z+bcuWLVBRUXFU3/711z8DobGxEeLx+FGNlc9//vNQWloKv/vd7+CHP/zh350IIrFo0SKoqqrqy6r/NM8//zy88MIL8POf//yw38DNmzcPHnzwQfi3f/s3uPjii4+43Wpra6GyslI0gF+9evUh/9A6SH19PSxbtgzi8Tj644uOh+7ubli2bBksWLAAJTdJY/pQ9/Dyyy9DJpOBl156CX0DfKifa0877TQ47bTT4D/+4z/gmWeega9//euwePFi+OY3vwmNjY3w2muvweTJkw/bvoerj6L8d6MaQOV/LS6Xi32LtWTJEmhubkbHpk+fDs3NzfDSSy/1HUun0/DEE0+guIkTJ0JjYyPcd999EI/H2fX6e3PBrFmzoLm5mZ0X4BOdUyKR6Ct///vfhz179sBvfvMbeOCBB6ChoQFmz56Nfiob6P0NFqtWrUJaqb1798If/vAHmDZt2iE3KNOnT4fi4mK48847RT3UwTZLJpN9P/kdpLGxEYqKisSfBw9SU1MDEyZMgN/85jfI+uOjjz6CV155Bb70pS8dyS32cXDT+Pe8DWLWrFmwatUqWLp0Kfu3aDTKNJafJhgMwve//33YvHkzfP/73xe/jX366adh9erVR1W3VCoFzz//PMycORMuu+wy9t+1114LsVgMzQkJt9sNN910E2zevBn+8Ic/HFVdLr30UvjjH/8Ie/fu7Tu2bNky2Lp1a58dz6H40pe+BPl8HlmxWJYFP/3pT1HcwfFJ21HKxD5U30vn6OnpgSeffBLFdXd3s+sc3MgeHMuzZs0Cy7Lgxz/+Mbt+Pp9H1w6FQv9j3kqiFBb6DaDyv5aZM2fCHXfcAXPnzoUzzjgDNmzYAIsWLYKRI0eiuHnz5sEjjzwCV1xxBdxwww1QU1MDixYt6hOhH/wL3Ol0wi9/+UuYMWMGjB8/HubOnQu1tbXQ3NwMK1asgOLiYnj55ZcPWZ8rr7wSnn32WfjOd74DK1asgMmTJ4NlWbBlyxZ49tlnYenSpXDyySfD8uXL4dFHH4X58+fDSSedBACf+I+dddZZ8KMf/QjuueeeI7q/weK4446D6dOnIxsYAIAFCxYc8jPFxcXw2GOPwZVXXgknnXQSfPWrX4XKykrYs2cP/OlPf4LJkyfDI488Alu3boVzzjkHZs2aBePGjQO32w0vvPACHDhwAL761a8etl733nsvzJgxA04//XT4xje+0WcDU1JSArfffvtR3euECRPA5XLB3XffDT09PeDz+fr83wbKzTffDC+99BLMnDmzzyImkUjAhg0b4LnnnoPdu3cf9pvam2++GTZu3Aj3338/rFixAi677DKorq6G1tZWePHFF2H16tWwcuXKo7q/l156CWKxGEp8+jSnnXYaVFZWwqJFi+ArX/nKYc81Z84c+Pd//3e4++670c/VjzzyCESj0T7fypdffhn27dsHAADXXXddnwb1hz/8ISxZsgSmTp0KN9xwA8Tjcbj33nvh+OOPh7lz5x722hdccAFMnjwZbr31Vti9ezeMGzcOnn/+eabpKy4u7rOSyuVyUFtbC6+88grSPh7kYKLTbbfdBl/96lfB4/HABRdcANOmTQOv1wsXXHABzJs3D+LxODzxxBNQVVWFfsL+zW9+A48++ihcfPHF0NjYCLFYDJ544gkoLi7u+4NkypQpMG/ePLjrrrtg/fr1MG3aNPB4PLBt2zZYsmQJPPzww3DZZZf11eexxx6Dn/zkJzBq1Cioqqo6pKZWUQaVzyj7WCkwDmUDEwqFWOyUKVPM+PHj2fH6+npz/vnn95XT6bS56aabTE1NjQkEAmby5Mlm1apVZsqUKcxWYefOneb88883gUDAVFZWmptuusn8/ve/NwBg3nnnHRS7bt06c8kll5jy8nLj8/lMfX29mTVrllm2bFm/95nNZs3dd99txo8fb3w+nyktLTUTJ040CxYsMD09Paa3t9fU19ebk046yeRyOfTZ7373u8bpdJpVq1Yd0f0dyv5EanNjjJk/f74BANPe3t53DADMNddcY55++mlzzDHHGJ/PZ0488URmk0JtYD5dh+nTp5uSkhLj9/tNY2OjmTNnTp+tTEdHh7nmmmvM2LFjTSgUMiUlJWbSpEnm2Wef7bdNjTHmtddeM5MnTzaBQMAUFxebCy64wGzatInVQWqHQ/HEE0+YkSNHGpfLhSxh6Dg7iDSuYrGY+cEPfmBGjRplvF6vqaioMGeccYa57777kCXP4XjuuefMtGnTTFlZmXG73aampsZ85StfMa+//vph7+1Q/WuMMRdccIHx+/0mkUgc8rpz5swxHo/HdHR0GGP+/xiQuP3225ltzkG7HOk/Oj4++ugjM23aNBMMBk0kEjFf//rXTWtr60Cax3R2dporr7zSFBcXm5KSEnPllVeadevWMRuYffv2mYsvvthEIhFTUlJiLr/8crN//35mH2XMJxY+tbW1xul0ovq+9NJL5oQTTjB+v980NDSYu+++2/z6179GMe+//7654oorTF1dnfH5fKaqqsrMnDkTWSgd5Be/+IWZOHGiCQQCpqioyBx//PHmlltuMfv37++LaW1tNeeff74pKioyAKCWMMo/DIcxg6AEV5T/hTz00EPw3e9+F/bt2we1tbWfdXU+UxwOB1xzzTXwyCOPfNZVURRFUf4BqAZQKQio11g6nYbHH38cjjnmmILf/CmKoiiFh2oAlYLgkksugbq6OpgwYQL09PTA008/DVu2bDmi96IqiqIoyv8VdAOoFATTp0+HX/7yl7Bo0SKwLAvGjRsHixcv7lcAryiKoij/F1ENoKIoiqIoSoGhGkBFURRFUZQCQzeAiqIoiqIoBYZuABVFURRFUQqMASeBHHR7VxRFURRFUf5nMnTo0AHF6TeAiqIoiqIoBYZuABVFURRFUQoM3QAqiqIoiqIUGLoBVBRFURRFKTB0A6goiqIoilJg6AZQURRFURSlwNANoKIoiqIoSoGhG0BFURRFUZQCY8BG0AOhtGIIKhtjWIwDHOQIjzEkxDh5jPS5o8FhD9J5ycfoPUhIt+Uk7SO1odOB9+3tnV0sJh6Po3JRcRGL6e3tReXaWm4e6XbRIcL/ZnAM4F4dJCgaT7AYKx1nxyjnfH4WKgetFIsJOfKo7BQq6HK5SAX5tRyknf3FJSzmuttuwp8JB1jMmyvfQeUtG7exGH8ghMojR45gMcteWo7KsbZuFuMhtxUI834vduF2bo12sJh9iRwqu5w+FtOTz6CyM5tkMeOqKlE5WcTHTwn52I6uGItJWHgeBEv4fY0aPRaVIwHcF7/4+X+wz1BuuGcpO5bJ40XCEuatTQaQwyn9bd3/2kfh8w/AScautNbQ+eZ08iAaY4T6uN34WLkwnvw+Dyp39vL+S2YtVHYZtvCCE3CMz8vHHH1+JFNpFpPJZlE5Z/FrWQb3D20LAACn00XKvE9dDhzz6E1fYDGUXyx+mB3zePG9B0NeoT54LLhdvL88ZAHo6uJrBF37PB4+xuwcXkMrK3m/x2J4De84YLGY3dvw5H5v7U4Wk07jdWTU2GoWM/4EvLeIp/iLKSoq8frszPH1OhfF6+zm95pYTEdbFJU/d9JYFjPn6xejcmXEz2KyNm6fA/FOFtOewc/gnlgvi9m1azcqf/uq77GYo0W/AVQURVEURSkwdAOoKIqiKIpSYOgGUFEURVEUpcAYVA2gm2okjlamR7elogbwKJBO4xikc9NLCbocmxyTbotVxxaCiM5j1869LGTpX1agckV5KYtxE+3HP835CotxEn1PMsl1XraNNTbhohCLoVLGRIqfxz8QLSE5Ty7PdSeWBwd5vB4W4yJ6GlnHSHRdVDcIAPTqpYLW8rwZ56Ky38v1IuvXbUblzrZ2FjP6mDpUbrbzLMbYuEapLI+xiP4qmcqxGK6t4jGpHNbu+IS55CFt5hb+5KTzIijozMpKsZawYmgNiyktLUflogDv9/5wZrk2NRPHbZERxlzeogOIjxUuFBYiSBs6BX2Wy42P0fYD4Jo2aXi7Pbh9PB7eOUVFeKzmrQyL6e7GOtxsno+5sBfXwCs0j9uNdW9GWEQzaTwOpW8wXG7S78K4dJAulOa/y0V1lMLVBiKAJlg2b0OPk+hMhXXf58f35XXzseF244YtFtYjD+n3mKDZJMsIpFO8Ph0H8LzYv5frsVtbsN44GecxeTKfNn+4ncVk0z2o/Pmp41jMkCFhVN79MdfGf7QOPyvbm3l93OTZIMhV2fzq7eX6dU8At5lL0POOGjUKlbt7eZ17evmzYLDQbwAVRVEURVEKDN0AKoqiKIqiFBi6AVQURVEURSkwdAOoKIqiKIpSYAxqEoghDqk0OQBAENJKQmiaMSCpMEVZ8+GRjan7rQ7LYJBMQ6mZZW+Ci8k9JKHCI4hCgwEsuuZG1QCQxfU50MaFo68vX4nKJstF/F/5+iWozMTTAJDJ489t3rqDxaRSWEh77LGjWYyHnHvzlq0s5sRjR7FjDAsLhj1e3oadvdj8NNvBDTi9JDmBGUMDF+2GBPF/OoP7PW9xAXzj6JGo7Pd+kcXQRIz9e3lij5+Y8gY8fLS6XFhILwn7gYxVt5sb7qbT2JBUmMpgiEjeCMa0Vg7fl5Xn9aFDM1QUYTGjxoxH5dIhQ1hMjoyNsO/I/76Vcq5saiQsGAsDMYvOC0J/eh7xWqShJUNpOlbzho9Lm6zFlsVj2DomLKleknDmdvEg28J1DBcFWcyUk3EC03GC2a+TzOVYLzd5TiawyXNvPMtionE8B3vi/Dw0F2Eg5t5OF++L/FE8h9xCJpTbjc+Ty/H7ymTwffg8YRaTIwbOPh83lC4mJupGGIgWPY+X92lPdysqtzb3sJjeHjwP6B7hk2OkbPM1fV8TXtNddjGLGTYEr7O7N37IYlqa8bMgm+bPPJozOHQ4f0FCWRlOqswnoyzG48f3mo3xcbhzJ34pgAV83SgRTKYHC/0GUFEURVEUpcDQDaCiKIqiKEqBoRtARVEURVGUAmNwNYB0Oylo5ai3p6Sno9hCzEA+R8kLBqWJBDYkDoa41sFFXgouyT5a2ttQ+YMNm1iMn+j7KsrLWUzD8Fp8bX4p2L+fXmsjixlWizVS+QTXFpSXYh2D18+v1hHF7bNt5y4W4yaGpEPrhvEYolvasp2/FHwgGkCqKQNB4+YgZqgmz++LaneyOUFHlcE6nJSDX4uO5/wAdK9UPwIAMJroBK0cNyjdvxXrRZqbuU7Q5yF6ES/XCYWceB5kBbNoikvQolENoPRyeurBK53HJkJXQaoLRUE8L11G6FPiXusTNLb9IZs8k3KO9zE1P6Zm2xK2EGNRfWFe0Ny5iE5QWh9JI0qm8zbRBdIyAECCGABLOmp6zAg62EgA99fnGrmGq7g4QOojtA+ZX/k8j4kl8bzt7OH652wWC73oeQEAkhliqp7hurwuQYPYH8Egv3fbYM24Q3jI5HO4nVPMrB3A78eaP1vQ96VT+FngFHSddPyk0/xa6RTu5+4oN0OO9ZDPCWPeRZ5yVFMKAJCO4XZev5pr0U0G30fTdq79zuWIbpk6ggNAjqwjsRg3yt5KtPBWmusfwYXH3Z6uPTwmgq8VCHPNZibH8wkGC/0GUFEURVEUpcDQDaCiKIqiKEqBoRtARVEURVGUAkM3gIqiKIqiKAXGoCaBxNNYuG4LgmE3EehbkvgeiGGq4IZMhfVSgge9em8vF3Nu2vwxKo8VTIxLSbJES8sBFrOZnGfnDp4sEfBjgX5nJzdwpnLcVJwLQN999z1Ufmf1ahZTX4mNVv1CcouxsbA2L/RXKotFvN2CIHbYMGyUSQ1dAQC2bMEJDNt27mYxA4EaZRvgImyvDxsbSyJ5NzGmFpMcSNkT5G2YJ6JmyUg82h1F5bgglqaGrV7BwDVEDFzdQW7g3B3F4yUrmOnWVeDEELdgcEuF/VL70MEqGb/TBnHQrBngSQM+YVUaEsF1TtFkIAAwxLQ838sTafpDugfWp1ICGjkkmevScxtBEE9F+1IuCe2bgSTESUkOLNtGOI+THJOSQNh9CbMgTxI6jLBeO218zCkkXbmdeN76vfy+fCSpISAkinnJGuX3c7Ndi/aFMLnjyf6TfSjHNI5lxzZtWYfKiSRf90NkDXc5+RywSQIDNYYGAEiQZ4rLLSWB4HMnhHXf6cSfk5KI6Bg3QpKVg2xDAoLpdC6PY5p382dnshsnQ4aCERZzzDH4+b512zYW43DiOqdTfB3pJC8XCLiFe3fiZJtcjq/FLgduZ5+fG1NHe5Ls2GCh3wAqiqIoiqIUGLoBVBRFURRFKTB0A6goiqIoilJgDKoG8OVXlqGy18N/zw4QDVc6I7w0negYBCkI+IjhpaS9ShKT544Obgy5Yxc2Z9zZ3MxiqiorUHnvvn0s5kArNme2MlyfURLGOqaMoC0wRCsjaQA3EJPpA20tLKaxugqVg26uKXMAbucd27ezmG5Sx0SK6xG6e6KovFdow/c/wC/m3rFbMMUcAPEEbo+QXxgcNtamdES5SScdL9SoGoDrMV1Jfu9tXViLMmLcMSzG58NjvjvXzWLisV5Urq2rZTHVldg4/IRx41hM0y7c9n/5899YjNuL28wtuI37iEZKkPewsWoEt+FclswD4UXvtoV1nPF2rrFd+dorqNwb53MnQ67lzOO15Qfz/4V9huKSRHdEKwu2oGOiujzByJceMVIMPTQAv/sBmTNLGsABnGcg0I9J12JaMMHi3uHCzwtZb0ieDVIM0ZnmhNtyECNxr6BNpRp2qU/dgolyf4wdeRw7tpOYvPck+bMqnscaMkdRgMUA0T+nBbNoD9E/Uz00AECW6NX8vhCLCYWwvrA4xOtjJ/C543n+vPd4cBtWlJWwmEwGz3evm+ufe7pxna08f3aecurJqJxO8zW9o60df2biiSzmtFNxH6YTvL9ScfxsyO3vZTHdLvJskszr6YsoBhH9BlBRFEVRFKXA0A2goiiKoihKgaEbQEVRFEVRlAJDN4CKoiiKoigFxqAmgWzejpMTfH4u1KSGsm4hUcRksLg06OPn8YaIoTRww8sMSVjoahOMlzNY4B1taWUxkMZi0nyCG/m6ieC0JMgFsX4XFqnbOS6I7ezE99HVzcXuTU24jqlOXh9HFp/bCO3T1Y4TBrZ8yIdDkoi3/S5e52QXTorZnuKC2EwKi12rKyMsZiA4iOjaJYhmqQhdEttbTLnOleLUPNclmNf2EHPxrGBQ/LfX30Llfbv2spg//WkpKnvD3Aw16MfHHDkuts8TY3UjzC+HC38uGOAJQhFiOp3J8PbxkKYPOqVEA9z2oim3A5/bSnKxdBtpZ0l673Dh+ygV5mB/ZDLcWNzkyb0LJs8sIcbibWGRBAZbSl1jh/jf6LQNB5IEMhAGYigtxfDEFSGhgswlKaHCSY4Zh3AP5PkhhdAlweUWXiTgxudxCklgNrmW9G2JM3vk7Vw9hCd41Q8ficqJRJTFuNx4bXELyQE0ucVYvH7pHH6m+PyCKbeDmmDzmHQKP3ds4aUO2SSus8fF1yMvWUgsIVGEmUwLzW6Re927hycjesh6GBLM/btJu7Yd4El7FkkasoWBmMnj/Ufe4gk5WWLUnRGMu/0hvv8ZLPQbQEVRFEVRlAJDN4CKoiiKoigFhm4AFUVRFEVRCoxB1QCeddoJqCwbeZKXzAsvovfY+FhIMH00RLaUA8FgkujX7DTXKPTGcExvLzeG9Prw7/2RshEsJkX0ENJLrwMBfB9+wcgz78AahWUr1rOYrm6skYp2cKPj7k6sWzAlXHsRDGOtQ2VVMa+PwffROLSIxcSiWFvpFHQetdXYxHj86AYWMxAc7AXkgr6HHAr5eTsbpiXs/28hT4iboeaJtinaw/Vr27btROVkV5TF5IhmM+DmZqh5B+6vWIxrP5PE+DVCTMwBAKze/ajs9XEt0Zhh+KXpfj/vd3cEt4ed4Pce30zuXWhnLzF1dwtdUUTmhaRptSx8H2HB4LY/8sLfxLbBx6w8H3M20R9Rzdsn9etfA+gg13ce5Z/oXIY3IEdpfoiFCPdO11mLXytHtE05QS+WY3NZ0JSSvnAKt2UTI3hb0O5myHyDPD9RnjyrnIK2sbsX17GGTzcBQQfrxM8Gl4Nr00IBapTN2yeTxvo+2+IDiM8d3j5eP9ES5rm22bLwsXSa62cz5IUIfiEvgM6LeJw/g7NZfG5pftGXTFjCM3j3ziZUDoX5mk4/t27dJhZTXY3X56HDwywmlcV5ARkjtCHRBdMXAgBwo+zBRL8BVBRFURRFKTB0A6goiqIoilJg6AZQURRFURSlwNANoKIoiqIoSoExqEkgw0JYhCkJ9HOCUS6FGdMKxqtZIrru7ubC0Y42nAjhyPL6bGnCprwH2ttZjCFi5NphQ1hMXd1QVHb7uAA958D7bY/Pz2LWvLcOldeuep/F5LO4DfOCODlDtvaBCknEj9u5M8ENnIGIf3PCiEnEcDuHingyiTeIP+h1cYPrgWARUXNeMtw1WDDcm0iwGEPk7W7BCJZK4D1cUwzZNDVV5mLgZJwYgAr95fHi62eTvM7FJUQoLijg6W34PIJBKTExDZSUspgR445DZdvFE2lscv14ZxuLaQOcBAIZLjin2mhXhjd0xkNMuR28v4yFz50WRPL9UeLnAzxOzpOW/mzOEdG8sMyRZpeNoEmCgNMhGEHTzBDhNDZJpLMt3u4sGUG4Lwc7uVAfg9vMKWSuuNy4fdxCpo+HDF7L5uPbIvURpiTQ5JEDndyAd+lb21G5rZt3GO0vSYqfIGP1vu/OEGv0aSwhAcYBeG6PHnUci6mpxc8dr4e3YWc3fn41N+/mMV0tqByPc6PjYAgnodC5DgDgINsHv/A8c5AekvzJqTkzTfgA4PuGbJbPf5oE4hWewV4vfuZV1/BneVsrXseSKb63eGvlu6hcU8fX/ZIy3GY5J3/mOTz4PjJZPjbc1HF/ENFvABVFURRFUQoM3QAqiqIoiqIUGLoBVBRFURRFKTAG9cflnl7827lkBE1fJi69fzzPdF7CC61T+IOv/ektFtO5B9fHL+gP4llspls5pIzF0Drv/JAbQ+7bvA2Viyq5rqqspgqVLQdv/rXv4XO3NXNdVYaY/aaFF0hv2oE1LpFh41kMhIiGK8cNpelLwDOCuablxtqGdJbraXIW1rRlM1znMRCcpM28gpYoa+NzB728nenL6CUjaDZ+hZevp5O4L7xuL4sJh7H+8pSTTmMxVcMqUbmzg+tyAn6sM+nqEkxDiYZt2/qPWEyMGBk7hfnlIO0sDDFwe+iL5/lkThOj02JBa5lK45P78rw+eS8VZPE+NYDHnct95AaqyTTXi9H6WYKxcJ7UOS/MkxzR4ckaQIykp3MSPZahYjUAcBhiGmzzOttEqJjLcDP9LDGzBYvPWyfR6mU9XHu1fj2Zkz6+1pSXDkPlvM3PkyWa37ygAqTt3HyAa6/e2dCKyp3d/L5oq0qjyRLatT/cHm6GfNJJJ6NyMMg1tz5imO6gjvcAkM3h8RuPf47F7NmDzZBbWvaxmLzBz8VEnGuSSyN4LldVcQ1gV2szPk+C6+ksOi8Ezb/bjee7z8fbkM658opyFuP14jE15azJLMYQk+lIMb+vXXvw835fayuL6cni/qqq5XuLWFcMlYdUR1hMeRnX7w8W+g2goiiKoihKgaEbQEVRFEVRlAJDN4CKoiiKoigFhm4AFUVRFEVRCoxBTQLZm8AJC24XP72TiO1tmwtZqciZmpoCAHz4ARayvv0BF7sPDUVQuaX1AIsJ+ojJ8xBuYhwKYvF9iZ8LUGk2S1pIcsgksBjZ5eXmkS4ifHYJ914UwCadMW+cxXy8bRcqjz55GItxkKyYvMUF8BbR7FtuQbhOjhkH79MUMWfOCEL6geBwYKFv3nARuJ+Y59ZEeJ9S42eHYLhLsf1BdsxJTILzeZ4AM3QYNgmvrWtgMUXlEVR+ZekKFjOsfjgql1ZwcbubJIGEBRPRd+N4vPT2cEH+7q1bUTlt8+QNOk+zCZ6UYnI4scDp5n2RI8k2xhKMoJP4PF4Xl+TbOTzGvNaR/30rGYtnqfG6kBGTJYk1WcHsN0cSBgStO0uKk5NA8DGapAYA4HLi61vZGItJp0h/Ca7KmTQeK7k4N8o/phInLMSSvA3f24DX63R0B4vp6MJrX9pTwmICFXX4WoL5eD6Px4ovUstjaE6RYBoM+f6fVZaQ+NQfUpJDeTlOApPWI5qUZoCPQx9JcvCV8gSCkiK8Ho05hif/xJMdqJxK8fHTWNeFyssy77KYzR/yBBMKHc/CkGeG6AEhSSZJn69uvhY3NODxc9LECSwmGMRtuKdpC4uptnCCiZRAmSSm9x1dfL3u7o2icjgcYjEHWsm5j2UhR41+A6goiqIoilJg6AZQURRFURSlwNANoKIoiqIoSoExqBrAtgzWCUgaQIpkFu3zYQPFTIqLU1asXI3KTfu4EWOoFu9vc2muYygi5ppWlhtV+iNYR+Hzcv2BTcxQPUZ66T3WkMQFU8x8Csf43dyEcmgj1rRkhRdI9/RgbUy4hOvXjAtrJOwcN/vMk78RLMk0mGibHIJhcpZo5dKCzmsgWBauc0YYP0GiC/QKLzLnEjKu76HWr5ZgvOommsSNH21mMZ3dWEf1/gdcU1I3AvfprK/NYjHJFO6fjzZwQ/L9B7DOtW5EHYtxW6ej8uLfLmYxLQc6UVkyLc+SF7QHBX1okQ+Phd4E18FkiIYtL1juxrJ4jDltYZ76sXYnmebapv6QDJzzRK9KNYEAXAOYywt6MTIuB2IELen7nGR+uT2CGTrR9+XivL2cDryOOQTjbHcAH/ML82RUNW735hQ3Mc8X4/XnpFJe5zc243nhquLave5mrIfau5nPN+JPDpUjT2YxDqK/dgW4gbvtoLpA3j5eoe37wxKMzoEu84KOmg4FSSfoIOu1LSyzLqLnDXv4M6a4DI8NS6jPsKG4Qvv38Lm94rW1+DxCfagpPzViBgBIJ/G5u9q6WAzVdaZzXIcfjeF50bR3D4tJpvB6/eZbXI993IljUJlquAEAWtr3o3K8ne8/Kofgcehw8Xvfu4/rbgcL/QZQURRFURSlwNANoKIoiqIoSoGhG0BFURRFUZQCQzeAiqIoiqIoBcagJoG43Fg0awsCfWqmORCj07ZWbva5aysWbxZ5uIGilcTCZ2eOGx0Hibmv08nrnCdmth1tnSyGCnttqkQGgI44FrLGEjwmlcNd4hBMQ0vCWDjqd3ETU18Ii5rLK7gBbz6H62NyXMTrsHF9XIb3l5ccylpcJJ/P4DbMpHnMQHDRy/MmBDcxz3YLSmjazZLwmArwjeExNdXYELQozJNtMkSdLCVCbPl4N46J8YSc5hY85lcs/xuLaWrai8rVkQiLOWHUCFT2ebkA3ti4jtk8F1RTj2kv8D71kIQFS0jIyZJODYR4klVVaQUqd7Rz0+kkSUZyDiAJjeIR5q2XHJPGgU1vSzKzZR/jbcGE/QMwi7YzURaTjWPBucvLTZWdTrJeW3xcukj/eXx8rPSQhJPyGn6tWDdJOEvwGxs/FgvrWzK8Ps0bceJTXjAotkliRtue93kMSfAoGVLGYoqr8TG/nxv3TzjuOHasP4IhvkbwjpY6ng6g/s2iXcLzNZHCz8GUsNZEKklygo/PpVQKrwl5wfy8sQ6b17NMDQDYvx8ncPb08j6l95EX1ms6nfLCur9r925UfnvlShZz5pRJqFxbz1+i0NyK6+wJ8PsqLsV7kmgvX0PpFskjrMU+yaR8kNBvABVFURRFUQoM3QAqiqIoiqIUGLoBVBRFURRFKTAGVQOYZwapggaQ6GeMoAkCC//m3dnMjUWTUaxjOHHiKBbj7MWmoQf2cWNYr48Yggq6oZ4urPmLtvH6OIi5ptPP99axGDaYbGnjmilnoBSVbcF4NRXD9xUQ9Blhciwg9EU+hrUWuTQ3i3U7fahsCQa3QF5AHnALmgUiXJLqPCDIcDGSUa4bnzuX4e3spidy8vpQLarLzWPKy3F/NY4ewWKGEHPvbIqbhFNTcL/wsvNtW3ei8htvvM1ivB6sITmwh7+MPUjnnNCGhmr+LN7vVJcjdakhujx+5wA5orEL+n0sprQWv8DeBPhL7nt6sRYtn4izmP4weT5WbHLMSHOAiHmo6TMAN5mmpr0AAA4HPo/DyVvMTkdROdvDjWLzxFnYAJ/bmQw203VYgnk9udV4jrdpRzHurzovNxaORfGatSbF2/Crs7H5+fvvrmEx73+0HZVLy7i2OVCC546njOuqgmVYnxWp5hrAUDkeY8bm61pXtJkd6w+3i+tMLVuaGUcO1ZQ5hOdrawvWr23ftovFnD71VFQOBHmfWkQn7BJ0eWOIfs5tCZr/LH4uW4JW3yL6Yocwv3zk2R0p41rUSAQ/70tLua4zQPTz9Y3VLGbNe3hdTUa5jrKOvLChvKKCxXR3RFHZ7eUG14Hgf9/3dPoNoKIoiqIoSoGhG0BFURRFUZQCQzeAiqIoiqIoBYZuABVFURRFUQqMQU0C6SIGyaEwF7J7vVgAm8tyc8R4Bh870MyNl6lWOxQRTDGJ2Dae4yLVrl58olSWC6qtNBY+RwLCfbmw2DaX4/flzGOxazbNhaNUh+3xcOFxPIqFosGQYB7pJkkXDp4EUkySNXJ+blBqHHiI2F7BoJQkmAgaZwjTg0LSxUBwEdNgX5DXOUsMZI1glJ0nDtJO5jANYJPEB2ee33s2T5IcLC5OLiLzoDvNTYyDflyfCSeMZjFvrViOyiEfv3cXSeigCTEAAGlikNrRy+vjJyJrJzUoBm466xSSSaiZNghGy7SOKcH8vKmtA3/GxRNFrAAWqjskM91+8AtjF4gI2xZMX11uXJ+gMC69fnyfTmFOtnfjZInmNr4eJcn4zhie3GYRU/dMpoPF5LLEcDfLExHyOXysaii/r3EnjUPlXR/xxIim1gOo3FsVYTEJH26PYBUX8Q8fj5OBXEU8OaFsKDFnr+QvCQgV4WNOP18jMhZew+OCkW9r0252rD+OfFQeCmG+9R8CnR34+SG92CBJTJ69POcK/GS+jR07ksV4yXkO7D3AYhoacJ+OPZGvfXk3noMpIVlr+DCcdFFUxMeqbfB4LiuPsJhMFidMpfM88ckfxnO5u50nrnST57THw9esbBaPhpZmPt+H1fM6Dhb6DaCiKIqiKEqBoRtARVEURVGUAkM3gIqiKIqiKAXGoGoAc1GsKbGpKyUAuEL4d3AXe8E1QC6Lf0/v7ea/wXvcWPcWCPHf18NhbBL6cYDvd/e1Y21MRjCh9LuwHmpYVSmLGVo5BJWDbi6aKDK4uT1urmPoJS/C9gumuD2d+D66cvw8gTDWZ5RVcMNUKuvy+fi1MsT01soJZrGkn6V+Z58RtCkDoaqmBpXdbq5/9LroC9ElzRbW/Hg8fCrs2rEblUtKeRtmiV5NunMf0a9UlnJNkoe88NsW+tRBBD1eybya6PtsQZfn9uM284a4jirbg3VlXuEl5VRgZAmm5bSjuUISIEVfYC9oAHMJUh/JaJ0YmQePQmx1wXnHs2NUq+dwc6FggJih5wTL6+1721B55w6ulaPmzLaDr0eeENaUunyVLMbO4bYwlvAiehv3qcvmMUVF+F6nn3ciixnX0IDKm1dzY2Hbg8dz6TCuo/5g+1pUDhbzMTfp/M+hssPP54CDzDdJ1pkmOvOuXq69isexjrI3ztsn5OLH+oOulwCCVnZASkEe4yRa62hnD4t5/70PULl5bwuLqTsGG9pPjHA9podod4cOr+Ex5F7Lh3IzZC8xma5tGM5iSiqwUbdl85WErlGdHVz32tKyH5X9AT7G9u3Hz+CMoAH0Eh1wXQO/915iRO9x8udrGTEydzh5fVKJIx9jA0W/AVQURVEURSkwdAOoKIqiKIpSYOgGUFEURVEUpcDQDaCiKIqiKEqBMahJICOH4EQIp0sQoBODYiPIwvM+LGQNB7loPhzGxyKCQL+qGJ+nqIyLk7t7sbi8ehgXqZ5+KjY6XbNqDYtJdWKB99DSKhZjHFgo7hUSPHzErNrYXFifiGMD6ZiVYjGTpoxB5Uh5mMWkiOGlSzBMzlLzY8EwGUgd84bXmZkGO4/ub48xY/B92TYfY5aHtKFkYszqx69VF8RtVlrCx2FPHLfh6nffYzFh0s8JkugDAHD5FReh8vatH7OYNOl3p3DvbpJ0kRUSM5xePO2DxXxsdPdiAbPl4FJ6Q66VzvPEB5rQkRcaOkeSSdKCIbGD1FkS0udJckYuLySl9MOokVzMnUnjNSIviO87o7hv/vbuOhazfst2VI728HlLz+x08znJzb6FpALSXl7Bnd1LjJeLS3nM2HHYXHfkcVyg376/G3/mlGEsZpQbr6uNx41iMf4ASRD08nmbzRMT8zYu9E+QOSm5ISdSuO2zgrGw14MT+bJp3l+RYp4Q2B8HWtrYseqhOJFHyINidyGtWQ6S8LZv734W8/77OAmkq72bxVSU4/ocO+YYFhMZghNDghG+jgwP1qNyeUM1izFkPHuEJCv6aHI7+LwA8twpEdZrA3gc9vRGWcze5j2onEjz9ToUwSbTDmF9zNh4TPkEs3GbrC0O6dkpvIBgsNBvABVFURRFUQoM3QAqiqIoiqIUGLoBVBRFURRFKTAGVQNYEia/izslkQL+jdvp5L+LpzI4plgweS4rxfqMsgquxfA6sYmqy89NFsPEPPK0qRNZzKlEAxiq5C+Z/uC9Tai8n7xwGwDAQWQm3ohgYpojxsJZvke3iECksjrCYiafdTIquwQfX0N0C25BA0jNfSWTZycxBJWMV22qZRQMigdCR3eUXp3FZJy4oW1BA0jvVQiB4XVY/7R961YWc6ATv9x83HFjWExzGo/DVW+uYjGnfR6Puz//8a8sJhbDujynML88LtwXbi/vjeIirNUJhLlWZj/RWuUM10g5SaMZSWdGtZ6CLM8iYyon9IWHaIdzwoksop+xBe1Xfzz3/OvsWC9p994U14LtbW9F5a4YN+DNEo2k0ylonZjGVjCBJbflBN43xFccIsJaU1GJx0HN0AiLqa7C62wi3sti3MW4zieeOYLFUGPsvOHa73QKt3Oim7dzLIm1lr09CRaTzRIzdEFQlyXPmFpi5A8A0LIVr+G9nVEWUzW2nB3rj/1Ne9mxmmqiPRfWR5uurNKiRW413sn1a93kWFs379Pt27Fe3s7xceih1aGDDgDAhx88ziA3AGd3Kjxj6CHL4uMn3ovvo6eXjw2L5Bx0xDpZTCyL524qwzXJObI+ZjLcsD2VweNXMvevLMFaS+mx6PYc5VsTBoB+A6goiqIoilJg6AZQURRFURSlwNANoKIoiqIoSoGhG0BFURRFUZQCY1CTQFIWFopKOSBOckWPYCzs9BCD0jAXS5eW4kSMsrISFtPahE1Ck0kuwqxuxMav9cdwMXA8j0Wz9WOHspgKYiAdb+fC2kQPPk9OEPGue38bKu/4qJXF5CwsYB41vJLFlFVggXc6285iDBGYu5xcWG8b3PZG+puBmTxz0SrxDAbL5iLegRDLUpNXDk1Ukcy0HQE//ozFY/bswkLoVIKLijdtwsk/2z7ewWJGj2lE5dJK3l8fk8+ddMoEFvPXvyxD5aIIF1Tnifmxx8On+LZN+FrtbTxhKRDG8ykQ4IlP8Rgez70ZLhR3E7PqEuBCcWOwgNpfwu+Lir4tm89l28ZibYd95MvbsuVvs2M+KmT38aS0PS1NqOzx9S/Qd3l5Wzg9+HNWji+iJM8HwkV8faxvwGPshOPqWEzDcLz2lRb7WYyTKP1j6TiLaYvitSUa5/MklcX9lRRMlVMpPH7SGSFRhIwxWzD7zpM2y2aF+W9IO8f4tSo9OMFjN0n4AgCId/B1tT8aG3hf0BXTISQwDcS8ngb5vXysekjiI+0bAJ6Y5XZL4xm3mZiq4CCG5NJ5SKUlk3eWRChAkxElI/j2Lpz0sXv/PhbT0o73Dak0bx+PB89dI5jyD63BL4Nwu/l6FCRm45k0TyYpKuLr4WCh3wAqiqIoiqIUGLoBVBRFURRFKTB0A6goiqIoilJgDKoGMO0gGhzhpelAtFYuyaSXmLwGufwIKiuLUZn6zQIA7NrVgsqd3VybMqkBa/4Cgt4wk8efswUTWl8R1loEwxEWY1n4t3zbyd2ZW3qwCeV7a3eymFwK6w0CIX4ey+CXTEt1pm0m+Yp63fi+qP4QACCXJboc4e8KJ3G4pKa9AyUQxDolakYMAJAjhpsZQZuWzeD24Qa8AHt24/HT28t1naEQNlGOC/qnXTvxy8XP/uKZLOahBx5DZZ9gqmqRtrfyXLdEjdWpKS4AQCwWRWWqGwQACIexhrS+bhiLcZA+7enh7dMdxeM5I+kxqXpIcEOluiDpL1cHNfwVDID7I9rONUFFJVgLZkky1Dg1Mea6ISCGrsXlERZC/WRtYYn2BfHdn3BiA4u5+PLJqNw4gseEiJZxfyvXGze17EflvW37WcyBLqyDy1u8//JkTkq6M7q2WJK+L0/7mF/LRQyT3Q4+B3Ip/KxKW9zsd9JJk1B5b0cHiwmYI/8OxRPkWks2UiX9PDlmhDXUkLlUTXSeAACjxoxE5Z1Ne1gMXa9FvSGNESrNekcQCkp6R34pHEPrBwAQDmM9nYuKZQHAduKx0NrJNxdOg+dFKsb1qr4y3If1DdL6iMdUwM/7PU8MyZt2N7OY9iOXmQ4Y/QZQURRFURSlwNANoKIoiqIoSoGhG0BFURRFUZQCQzeAiqIoiqIoBcagJoHkidhWFKmSYw7B6NSRxceoCSwAQFEYVz0viN0PtEZRuaS8iMXUjsQGzhmbCz6ZuFUwTM5YRPwv3Ds1s5QSV6gRrNPDEzxIfgd4fTzG4SLtLPguG0PMLAVBs00NmwUxMDW4lPrL6cLCbKd9dH97pFNYJS+Jk6k+WDKmzmRoooiUCIETPBKCEbSH3FdRUZjFFBfjcefz8v7yuHFfJOJ8HAYCOIlISlyRkmIotH+k5I2iSnytmqoqFpMlQv6K0lIWkycdtLebK5p7W7DwOdnL29lNx48gAqfT0s5zYX9/nDlxLDvWmsD3uXE3F8173Lg+OcE81uXD88Tn5QavtUOwSbCxeVJaT0cbvrYpZjHFxDy2N8bNvte8je/jpT+8wWISOZxA1TCOm5jbXjyesjlp3afriJDoY+Gx4nHxeeIm7SwJ9HuJ4X4my5PAbPK8qKrgIv6gH8/lETV8DpQVHfkj1C0YgFMcwsKWIAlm0vz3kUSD0iF8Tp52xkRUfv+99SwmmcCZTlFhjSgfSl++MICkKyGEPl8lI2iKQ3p4kmduhmZUAYBFXlbhcnGj7EwKn3vPzjYek8bPhooKnuDRsh8nTNUOq2Ux4RCep+UVfI/S1NTEjg0W+g2goiiKoihKgaEbQEVRFEVRlAJDN4CKoiiKoigFxqBqAG0L616o7uuTIPoZySwa70sl8+FQEXlZtUvQ3BGDyYYx1SymdAjWmeRsrhtwOalOQNAtOokJtqBRsDJE65DjWjnaYmWCbjFJpDEeD+9GlwufW3qXdj6Hr+YQXtRtG6KjEnSCVAMomXvTNmOmvQOE6s5cwhhzu4leTOgLqsuj5tEAANFubGJcWhphMZks1iBJ14oSM+Tubq6noQ6pPh/Xh9HP1dTw8UxNr7OC/qm1FevwHEJ/UU1iff1wFuMlWsYDrVwrQ42gK4uobgggT8ZCLMt1XfEkPhZL8Xnqd+A6O2zBjLkfgsI6Ygw2eQ4EuN4nTcaPu5jr11zkBfK2OAeIdtfJ+6+YSNGCFUIfd+5F5eZd3SzmL//1Hiqnevh5DnR2onKXMHYbT8K6QDouAACcDjwnTZ5rAG2iHYzF+LWSSdzvecHAmepg/YKOOk50gmALZvrk2VRfzTWAkYDwjOsHv2AEbVNTd2Fs7NuN+1R6vA4bWY8PCGbI444fjcpTvnAai9mwfgu+9j5uUDxi9FBUNoIuFxxk3ecRR4UkE6T93h2Nsph3Vq1B5U0f72AxWz/G7ZyK8ed0OoHnioNq5QGghJjpp5N8zcpmsa4zGOaaxLHjGtmxwUK/AVQURVEURSkwdAOoKIqiKIpSYOgGUFEURVEUpcDQDaCiKIqiKEqBMbhJINTkWUg8cBBTXmqkCwBgXDimpIaLZmnigz/C97JD6iOoXFpaLpwHX8sWDEqpMDuf48Jjrw9/zu3h90V1vR4Hb363F1+rrJKLk4NBkuQgiIGp8WpOMsW18fWNIDx2uWl7CAauJMOEJoVIuPxHN/T8fiySpUkPAAA90f4NUymxWJwdowbJkqG0x9u/CNxBxNGbNm5mMV1dOFlCMjGl5tUpIRHCScaUFEP1ytXV3Nw3UoYNZDdv2cpiaofiJJRQUYjFVA/Bwvl4nJs8V3Vjk+KueIzFUEPiDsGUOxAKorJfEMD3x45mbpicIokhI4cNZTHpNG7nrp4OHpNNknIPi9mzH99XcTmf/yecjM2iR47niTWJFD7PG8s/ZDE1FUNQeeznj2Exa9auR+UNO7azmIoGnFBVEuLrSC6D1590iq9HeZJESM21AQBCQdweeSF5I53AA7y3I8NiWndGUbnE3cliEkmcOOMT6uP29G/qTLGEDAa6RqRTPBFqXxNOTigqCrKY8ho8l30hbkwfKcXG4VOnTGYxXmKQLCY1kueF9Oh00uQ/5wBMnoVkEmoW7RCeQ7SOAT9PqDhw4AAqN+3myS3dnfhZ4BWe00OH4LkzpKqCxRwgz0Uh5xM8XnwfISEJxDuAZ8zRot8AKoqiKIqiFBi6AVQURVEURSkwdAOoKIqiKIpSYAyuBtDOkzKPCQaxwa304ucc0YKU1fAXWlPv2miS62mCpfha8SzXQ234GL8QvaeXnyeZwBoSajgJAFBZVYbK5RURXh9iIFse4S9xD4bxseIyrqsKBHG3OQWNG9VRWJYgQLCoTpAb57pdVGsxgGu5JI0k1upIZrEDoasT63I8wovVbTLwJDPkTAb3qVSfSKSExPBrUf0lvTYAQJ7oLzs7uSlvmtTHLxhBu1z4+qEQ1wBRA2m/j+tnqXk27RsAAB8ZU9FersuL9mKNpN/P61xXjbUyZWE+5k9oHIXKHVE+BzuILrAowzVSaaIHdR2F7WyX4RocyOL+cya4XrSyYhgqRyKCNi2PdYHprGBUTYzWfUG+RMcSuC227d3HYuJtuH16DvA6T/3SVFSuqOBaUCD6px3CtXo7cH+ZdP+a7ZIIN7j3OvH4loxz24k+K9bOtaDpKP5cNMrPk0ritt/n4drGPduwZrxhODeCPhoNoC282MBJdObdXVxD2t6OjdYzKT6X6uJ4DoTDwiOe6M5GCUbD4TJ8bn+Qj+c8uQ+XoP2mWnRpT0DXI2lPQDV/hr5RAgAc5FrlpYK2OYyPJWMf8POQfYxT0PP7yPNi/74DLIbWUXq+eovwemMc/N5tyWB7kNBvABVFURRFUQoM3QAqiqIoiqIUGLoBVBRFURRFKTB0A6goiqIoilJgDGoSiI+IySVBPBBDRynG5cbCWr9gZrlj+25U/mDdFhbjMPhzLfu5yWvcxqLiZCbJYrJEBO5ycqG4242TSUKCcLSIJBE01I1kMQEvTvpIxAXzYdI+kvFyOo2TCizBCJrmd9D+AwDwevDfCFaOi1SpIFYyDaX9Hu/lovSjwRaSW3I5LLaVEjwCJCEnmeRJBbRdvV7ezskUHi9O4d7zeVzHcJgn9lRUYMG51IbJRP/X8nnx2AwEeaIITdqhSSoAABlyTOpSB/n7MZ3nyTbbdzehctjF+6K7A5uohor4fC+LRFC52DeExcRiODmiq5snk/SHy8f7JhHF68a+PbtZTFsxjhlayxNiAiW43V1+IRGKCOAdLr4+Jnrw3Nm/l4vLO3bgtqgpqWYx1KTb6eR1rizH7Rwp4skbJSV4XowaXc9iqIi/eT9PcmjesR+Vkx08eSPgwW02fhgfB3Un4mO2LZgYm/4TGKpIYl9JMb93aoI/MAShP3Fn37N3L4vpIuMwFOIG4OEQrmMuxedkjhr3B/g4rKnHbSilIZgBJVkRs2jBB5omfUhJIBaps6Fu9gBAffqdDp6g4/eSNrOElzG4yTNPuBZNgCmv4AlCiWQUlUuCfG3pSeJEulyCz+Uw8M8NFvoNoKIoiqIoSoGhG0BFURRFUZQCQzeAiqIoiqIoBcagagDdRC/mcgrGuURMRE0gAQByRJLkEN4yvW9PCyqHBT1EcRDrXpr3cDPbANFMNIwewWJiRHNzoJUb+eay+D6obhAAoDOONVxtLVy36HPh3/tdwh7dCVjbFOJ+oJBOR1DZ6+O6RYvIQySzaJsYU9o5rnnJEs0G1SgC8H522P2/FFyCGspKWjmHA99YIsHNYnNkkNHzAnDtXibDr2WYlojHuN343qUxn0nn+o1xOlykzK9Fx53DmWExVAPokcxsXUSXI+iW8kSX45TeBk9uI2rx+iSa8QvZPYIW1U0M5MsrueamiOjT6mpreH36obOtnR3LJfH8twT/5q5O/Ll4jM+TmlpsaB8p5RPX5Sd6Yy/vY5eN2zkb59fq7cT6uTGjylgM1Yvls3z+p8m9S/OkKIDPs39XG4vpbOpEZTcdGABQV4bbZ/TxtSymthrrRcMhbjrtICbmku6MyrqcLj4HDDHBlox8j2YZczFzfYD9La2o/N7a91hMMonXsWLhRQL0DQkbNvJnTLAIP2OGDuPzxEc07JLhPl1HjhaaByDlBdD1WtIA9sRx+2zcuJvFbNy4jZxX6EAHXn+Exxl4iH63srqcxUR34D4NClr0IvK8jyd4DgLV8w8m+g2goiiKoihKgaEbQEVRFEVRlAJDN4CKoiiKoigFhm4AFUVRFEVRCoxBTQKhenxLUEtT0aVTEMRS0W4qzg2cq6uwWWxFxVAW09WOhaJ1wypYzKgx+Fj9iEoWk05hEWbLfp4EEu3C9xpL8YQTcOIba2/l5sOte6L4I4LQPxHH5pHtbdxU1RiczCKdxyYiXltQNGfTOKHCCKJZY5FEESEBhpoqe5xcBD4QUiksbpdMQ6mIWDJ5zmTwffn9PEmGGkoHg9wodyC34fdjobrUF9EEHi9O6moKABYxH6XnBeDC7FSK37sh/SyJub1E5Ozx8aXCSRJVJPG2RZKI6Bz4pAK4PbI2XzcSHVjg3drayWKKiIH0kFpuftwf+TQXYedzeP5LSQVZJlLnbZGM47HrFxKzAm4yJ7mPL+QyuD7pODdMjnXi5A33WN5/sV7cplnh3nPEGD8XE9asTXjsDq3igviTR2LT+xH1PPGgogInNUjJSTRZwxLWLGrcK61rdFjmhQQ4ekxca47iOxQD/FofbdiIyjt37GQx4TA2dT/Q3spidjTtQOXlf3udxQTIeUaPaWQxI+obUHnkCG7uzRL7hHWEtpn8cghMLsvn//btuD0SQrJEiphev/TiMhazcQtuH9shrVnk+SX0eyaP52BnD9+j+EN4fmdp1iUA2Kb/hDz6HBpM9BtARVEURVGUAkM3gIqiKIqiKAWGbgAVRVEURVEKjEHVAFJTXrdgGko1ZdLv/R5iphvgUicYMQK/rNrr4noak8XamNNOGcViykqISa/N9TRlJfjctUXDWEwug3+778nFWUzGibUF3W383ldlP0Jlr4vffLQL6xaKi7g2zefGdc5lBC0YkaK43FzQRk2Mpfd/u6kWRAoiMgqXoIMbCFRTQo3FAQDyedzOklaOmmvGYry/qMZFNMoegMbF6cRtHwqHWUwggPtQ0i1SDaDkw0rr43QJRuJEd5vLcm1KnLRHyOYvJA8QTSTVVQEIkj+hzlnSrpKWyE8WAcmQPEZMi+M7drCY/nCBpAnq3/CWdns2w8dKaws2cI/18vkfCuP+8gX4uuYjRtkBPzeYzeJLQWdnlMW0tWPtsItqnwDAtvE8OXE8N2duHI5109VDuAYwFMJjRZC4gu3EzwsjOPAyQ3JLmG/kew1bMA2mXeoS1hFDBqsl6dck8/N+SMT5WvPBBxtQuTfWy2LCYTwH9uznOsF4Dnd8Is21sruasRnyjt0fs5gzTpmMyjVDuPF6mGhupXl7NGbR+/btZ8defumvqJwRzJEdDtw+H23aymJiCdyuxRV8foWDeBxGu3pYTEs7Nq8f0s3nYFk5NkhPCrpFQ9Yb9ryFgekmjxb9BlBRFEVRFKXA0A2goiiKoihKgaEbQEVRFEVRlAJDN4CKoiiKoigFxqAmgfQmsLGoy8dFvC4irHUKqnAX3Zc6uVDTJuJfk+NC30gYX9/vC7KYbBaLSXMWbxKvjc9jBAE6GHyeMNeEgouYxfpKeNDwKlxHY/F7Hz+qDp8nwOsTcOHzWIIBbx6I+F8wvHQ68L07hBHjISbPXhe/LyvXvznrQKCJGVQQD8DNNNva2vs9b0DINKICZikxw+uj9eH9RZM3uruiLKY0UtpvHWnCC012AQAIErE9TcwC4CbFNMECAMAi5qNSYg81AJcSchxkTIlCetLO0sgwpA2dbn4tn4ckihyFeDovXNwmty5d203XDb4csXZPJriQPREn5sMWN5QPBPF9BgN8DsQTOJnt4493s5hTJ09A5fKyYhYTPYA/d+ak41hMaQkWu0v9Z5E1XUoOoM8CWziTTca8bQnG9LTDhEQNdn0pV4EapgtBzqNIcohGo+xYnCSGuIUx1hvDyQgpmyeTgBev6cESnlTgiuEYX5CvWR4Pvq+skKxJkZLkaDsPxCz64y3bWMzatevxeYTvrtIZfK+ZDJ9f4RL8bBg9fgiLiZTj9mhtKWIxJSV4DkprqMuN1+JAgD880xk8v6WEpd4oTwgaLPQbQEVRFEVRlAJDN4CKoiiKoigFhm4AFUVRFEVRCoxB1QAmyIvnHTluMOsikg2PILqxvVjT4hIMpR1U5yEYgvrc+DxWlmsCqBTN6eS6xQwxdXULJsYO4oLrcfE62xY+d1bYfleUYb1BTzevc0PDUHwtH9cNWEQrI0lVXMSN1cr3r5mSNBzMeNnNNSUuD+6LfF4QSQ0AqjORDJypVk/SytHzFBdznUeQGB13dfEXfieI7jWX49cqL8f6PoeD63IyGTxXJD0N1Tv6/LydqcOu9AJ7OhikPvWQ/pLOQ015JY0U1SAKUlR2brHOBNlwG19rIOehjK8rY8e6e7FOJ57k/ZdK4WPZDB8H2RwxmRdeRF9GxmFIWPvoGMtm+bWK/EQPFe9mMe++8QYqjxrBDe5H12ED4NKSCIuhhskuj6QFxfdqido9qn+UtNZEdyqMOf4pHkPnlzxSyEsCBF3e0RhBd3VyQ+BYHI+NDqL3AwCIEI1rbUkli/H5yZqe5utIZQUeY+XlFfxaZXjtc9EHt4C4RpC3DUjaxmg3XkPfe28Ti+nowHMwZwlruk3WQ8GZnupVa2r5fA8V4xEULuUvWvD68DG/oMP1k0SAVIqPTJPF8zse5zrzbFaNoBVFURRFUZRBQjeAiqIoiqIoBYZuABVFURRFUQoM3QAqiqIoiqIUGIOaBOImJr3pnGAeSRIo/G6edEHF5G6nIIQmxpSWIJR0WvhabuFaXmJiTI2GAQDcRCTvkhIhMrg+tuAE6ybCekk0P2IkNnkW8mhgSHUElXtjPDkhS0wwPV7ePjmSiOERkjecJGFBShigTrl5IbHH6yIG4EIixEDIZnGDJBJcUJ1KYUG118v7nQqWe3q42SZNunC5eZ2pIF9sH0IgwEXF0W58fb+fG4vSxAeXW0iOIvPLCEJoeh4poYLehZjgMYB7pcamohE0QTKUpsiC8/6NqfvjqoumsGPtUSzI7xXGXDKJx2UyxZO3enrxeXLC+jikvASV64dxob9N7jOd4osENcr3CWOltBQbPw8dJpjiRojJs5C8RZM3pD62zZEL2QdiGix/7vBl+Vj/Js/ytY88CaRHMPbt6CZruJDfVVyF+yJcwtcRjxevUWEXf/lBURF5SYCQJJdMRlE5nUmwmEA2jMpuYYzRJBCpT3fuaELl1Ws+YDGJNB7jLi+f27YTxxghHSiVwp/LJNMsJlSMnxdun2CmT9bVeIqfJ2PhNsvn+ZrgdoVQ2Ra+kwsGw+zYYKHfACqKoiiKohQYugFUFEVRFEUpMHQDqCiKoiiKUmAMqgYwR7QEksaFagBtEDR3RGtl56SXguNj9EXwAAAeYurs83BhBfWTFjyewUu0g/k0vy+LaBKzhmtlnB58Xz4fr4+fGExKXqiWjTVIDodkqow/6HTxG3PaVN/H+8IiTtnSy8+pGWs2wzVJeaKJdDmPTgNINSRSG1IdJzVZls4jGS9TXWA6zU06XaRdJUNpqh1Kp/h5qH5Gui/6QnaPl5uPUq3VQHR6Ugzt04For6QYmzqtD8CYWjrPQO7DEO3ZkdtAA5SW8v4rLcUaHJegLXQwA24+njJEJywZlLuJVtbv40s00ykLOk968x4XHyseD17XnIJJL+0+qj8EALDIImULLU8/J2s4yWfoxQGAyguFEKb8kmKMIc8haeySuSTFHM0oy+S4BrCsAveFwxfhH3Tg8ZNMC+ssfWAIYyMcwmM8I5jXx+K4jk17drMYK4/HalkZN5Sm70OQDMD37NmHyvEYb59IBM/BQAnXdXt85IUEgjmzh2gHLZs/y5NE7pjKce2ey0X0l8IgS2Ww5tcf5HM56CeabZv3RS4vJAIMEvoNoKIoiqIoSoGhG0BFURRFUZQCQzeAiqIoiqIoBYZuABVFURRFUQoMhxmIuhsA9u/f/99dF0VRFEVRFOXvYOjQoQOK028AFUVRFEVRCgzdACqKoiiKohQYugFUFEVRFEUpMHQDqCiKoiiKUmDoBlBRFEVRFKXA0A2goiiKoihKgaEbQEVRFEVRlAJDN4CKoiiKoigFxoCNoBVFURRFUZT/G+g3gIqiKIqiKAWGbgAVRVEURVEKDN0AKoqiKIqiFBi6AVQURVEURSkwdAOoKIqiKIpSYOgGUFEURVEUpcDQDaCiKIqiKEqBoRtARVEURVGUAkM3gIqiKIqiKAXG/wN3Du6rnRgavAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "test_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "    ]\n",
    ")\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "    ]\n",
    ")\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "L.seed_everything(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "L.seed_everything(42)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = torchvision.datasets.CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "torch.random.manual_seed(42)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=4\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "# Visualize some examples\n",
    "NUM_IMAGES = 4\n",
    "CIFAR_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"Attention Block.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of input and attention feature vectors\n",
    "            hidden_dim: Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "            dropout: Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm((embed_dim, 32, 32))\n",
    "        self.q_net = torch.nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)\n",
    "        self.k_net = torch.nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)\n",
    "        self.v_net = torch.nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)\n",
    "        self.head_unification = torch.nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm((embed_dim, 32, 32))\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(embed_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Conv2d(hidden_dim, embed_dim, kernel_size=3, padding=1),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    # def spatial_linear_self_attention(self, Q, K, V):\n",
    "    #     \"\"\"\n",
    "    #     Q: (B, D, H, W)\n",
    "    #     K: (B, D, H, W)\n",
    "    #     V: (B, D, H, W)\n",
    "    #     \"\"\"\n",
    "    #     B, D, H, W = Q.shape\n",
    "    #     Q = rearrange(Q, \"b d h w -> b (h w) d\")  # (B, HW, Dq)\n",
    "    #     K = rearrange(K, \"b d h w -> b (h w) d\")  # (B, HW, Dq)\n",
    "    #     V = rearrange(V, \"b d h w -> b (h w) d\")  # (B, HW, Dv)\n",
    "\n",
    "    #     Q = torch.nn.functional.softmax(Q, dim=-2)  # (B, HW, Dq)\n",
    "    #     K = torch.nn.functional.softmax(K, dim=-1)  # (B, HW, Dq)\n",
    "\n",
    "    #     def break_into_heads(M):\n",
    "    #         return rearrange(\n",
    "    #             torch.cat(M.unsqueeze(0).chunk(self.num_heads, dim=-1), dim=0),\n",
    "    #             \"head b (h w) d -> (b head) (h w) d\",\n",
    "    #             h=H,\n",
    "    #             w=W,\n",
    "    #             b=B,\n",
    "    #             head=self.num_heads,\n",
    "    #             d=D // self.num_heads,\n",
    "    #         )\n",
    "\n",
    "    #     Q = break_into_heads(Q)  # (B * num_heads, HW, D//num_heads)\n",
    "    #     K = break_into_heads(K)  # (B * num_heads, HW, D//num_heads)\n",
    "    #     V = break_into_heads(V)  # (B * num_heads, HW, D//num_heads)\n",
    "\n",
    "    #     # K (B * num_heads, H, W, Dq//num_heads)\n",
    "    #     # V (B * num_heads, H, W, Dv//num_heads)\n",
    "    #     # KV (B * num_heads, Dq//num_heads, Dv//num_heads)\n",
    "    #     # This should inner product of each channel of K and V\n",
    "    #     # KV[b, i, j] = sum(K[b, :, :, i] (H, W) * V[b, :, :, j] (H, W))\n",
    "\n",
    "    #     KV = torch.bmm(K.permute(0, 2, 1), V)  # (B * num_heads, Dq//num_heads, Dv//num_heads)\n",
    "\n",
    "    #     # Q (B * num_heads, H, W, Dq//num_heads)\n",
    "    #     # KV (B * num_heads, Dq//num_heads, Dv//num_heads)\n",
    "    #     # functional conv2d\n",
    "    #     # weight: KV[i, :, :].unsqueeze(-1).unsqueeze(-1)\n",
    "    #     # input: Q.permute(0, 3, 1, 2)\n",
    "\n",
    "    #     QKV = torch.bmm(Q, KV)  # (B * num_heads, HW, Dv//num_heads)\n",
    "    #     QKV = rearrange(\n",
    "    #         QKV, \"(b head) (h w) d -> b (head d) h w\", h=H, w=W, b=B, head=self.num_heads, d=D // self.num_heads\n",
    "    #     )  # (B, D, H, W)\n",
    "    #     QKV = self.head_unification(QKV)\n",
    "    #     return QKV\n",
    "\n",
    "    def spatial_linear_self_attention(self, Q, K, V):\n",
    "        \"\"\"\n",
    "        Q: (B, Dq, H, W)\n",
    "        K: (B, Dq, H, W)\n",
    "        V: (B, Dv, H, W)\n",
    "        \"\"\"\n",
    "\n",
    "        B, Dq, H, W = Q.shape\n",
    "        _, Dv, _, _ = V.shape\n",
    "        h = self.num_heads\n",
    "\n",
    "        assert_shape(Q, (B, Dq, H, W))\n",
    "        assert_shape(K, (B, Dq, H, W))\n",
    "        assert_shape(V, (B, Dv, H, W))\n",
    "\n",
    "        Q = torch.nn.functional.softmax(Q, dim=-3)\n",
    "        assert_shape(Q, (B, Dq, H, W))\n",
    "\n",
    "        # Can we do this without flattening?\n",
    "        K = torch.nn.functional.softmax(K.flatten(-2), dim=-1).reshape(B, Dq, H, W)\n",
    "        assert_shape(K, (B, Dq, H, W))\n",
    "\n",
    "        def break_into_heads(M):\n",
    "            out = torch.cat(M.unsqueeze(0).permute(1, 0, 2, 3, 4).chunk(h, dim=-3), dim=1).flatten(0, 1)\n",
    "            return out\n",
    "\n",
    "        Q = break_into_heads(Q)\n",
    "        assert_shape(Q, (B * h, Dq // h, H, W))\n",
    "\n",
    "        K = break_into_heads(K)\n",
    "        assert_shape(K, (B * h, Dq // h, H, W))\n",
    "\n",
    "        V = break_into_heads(V)\n",
    "        assert_shape(V, (B * h, Dv // h, H, W))\n",
    "\n",
    "        # K (B * num_heads, H, W, Dq//num_heads)\n",
    "        # V (B * num_heads, H, W, Dv//num_heads)\n",
    "        # KV (B * num_heads, Dq//num_heads, Dv//num_heads)\n",
    "        # This should inner product of each channel of K and V\n",
    "        # KV[b, i, j] = sum(K[b, :, :, i] (H, W) * V[b, :, :, j] (H, W))\n",
    "\n",
    "        KV = (K.unsqueeze(2) * V.unsqueeze(1)).sum(dim=[-1, -2])\n",
    "        assert_shape(KV, (B * h, Dq // h, Dv // h))\n",
    "\n",
    "        # Q (B * num_heads, H, W, Dq//num_heads)\n",
    "        # KV (B * num_heads, Dq//num_heads, Dv//num_heads)\n",
    "        # functional conv2d\n",
    "        # weight: KV[i, :, :].unsqueeze(-1).unsqueeze(-1)\n",
    "        # input: Q.permute(0, 3, 1, 2)\n",
    "\n",
    "        kernels = KV.unsqueeze(-1).unsqueeze(-1)\n",
    "        assert_shape(kernels, (B * h, Dq // h, Dv // h, 1, 1))\n",
    "\n",
    "        input_tensor = Q\n",
    "        assert_shape(input_tensor, (B * h, Dq // h, H, W))\n",
    "\n",
    "        # QKV[B, D, H, W] = Q[B, D, H, W] * Kernels[B, D, D', 1, 1]\n",
    "\n",
    "        QKV = torch.nn.functional.conv2d(input_tensor, kernels)\n",
    "\n",
    "        QKV = self.head_unification(QKV)\n",
    "        return QKV\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, D, H, W = x.shape\n",
    "        after_norm_1 = self.layer_norm_1(x)\n",
    "        Q = self.q_net(after_norm_1)\n",
    "        K = self.k_net(after_norm_1)\n",
    "        V = self.v_net(after_norm_1)\n",
    "        attn = self.spatial_linear_self_attention(Q, K, V)\n",
    "        x = x + attn\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = torch.randn(128, 256, 256, 1, 1)\n",
    "input_tensor = torch.randn(128, 256, 32, 32)\n",
    "\n",
    "QKV_all_loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_channels,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        num_classes,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        \"\"\"Vision Transformer.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim: Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels: Number of channels of the input (3 for RGB)\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers: Number of layers to use in the Transformer\n",
    "            num_classes: Number of classes to predict\n",
    "            dropout: Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        kernel_size = 3\n",
    "        self.input_layer_cnn = torch.nn.Sequential(\n",
    "            torch.nn.ZeroPad2d(same_padding(kernel_size)),\n",
    "            torch.nn.Conv2d(num_channels, num_channels, kernel_size=kernel_size, stride=1, padding=0, groups=num_channels),\n",
    "            torch.nn.Conv2d(num_channels, embed_dim, kernel_size=1, stride=1, padding=0),\n",
    "            torch.nn.GELU(),\n",
    "        )\n",
    "        self.transformer = torch.nn.Sequential(\n",
    "            *(TransformerBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers))\n",
    "        )\n",
    "        self.classification_head = torch.nn.Sequential(torch.nn.LayerNorm(embed_dim), torch.nn.Linear(embed_dim, num_classes))\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.positional_bias = torch.nn.Parameter(torch.randn(embed_dim, 32, 32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply depthwise separable convolution embedding\n",
    "        x = self.input_layer_cnn(x)  # (B, D, H, W)\n",
    "        B, D, H, W = x.shape\n",
    "\n",
    "        # Add positional embedding\n",
    "        pos_embedding = self.positional_bias.unsqueeze(0).repeat(B, 1, 1, 1)  # (B, D, H, W)\n",
    "        x = x + pos_embedding\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        pooled = x.reshape(B, D, -1).mean(dim=-1)  # (B, D, H, W) -> (B, D, HW) -> (B, D)\n",
    "\n",
    "        # Classification\n",
    "        out = self.classification_head(pooled)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(L.LightningModule):\n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "        self.example_input_array = next(iter(train_loader))[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = torch.nn.functional.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(\"%s_loss\" % mode, loss)\n",
    "        self.log(\"%s_acc\" % mode, acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")\n",
    "\n",
    "\n",
    "def train_model(**kwargs):\n",
    "    print(\"Initializing model and trainer...\")\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n",
    "        accelerator=\"auto\",\n",
    "        precision=\"16-mixed\",\n",
    "        devices=1,\n",
    "        max_epochs=180,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "    )\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    print(\"Checking for pretrained model...\")\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
    "    # if os.path.isfile(pretrained_filename):\n",
    "    if False:\n",
    "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = ViT.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        print(\"No pretrained model found, training from scratch...\")\n",
    "        L.seed_everything(42)  # To be reproducible\n",
    "        model = ViT(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        # Load best checkpoint after training\n",
    "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model and trainer...\n",
      "Checking for pretrained model...\n",
      "No pretrained model found, training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "batch1 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membed_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_heads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT results\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "Cell \u001b[0;32mIn[21], line 65\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m L\u001b[38;5;241m.\u001b[39mseed_everything(\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# To be reproducible\u001b[39;00m\n\u001b[1;32m     64\u001b[0m model \u001b[38;5;241m=\u001b[39m ViT(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 65\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Load best checkpoint after training\u001b[39;00m\n\u001b[1;32m     67\u001b[0m model \u001b[38;5;241m=\u001b[39m ViT\u001b[38;5;241m.\u001b[39mload_from_checkpoint(trainer\u001b[38;5;241m.\u001b[39mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:967\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m--> 967\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_fit_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    970\u001b[0m _log_hyperparams(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:208\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_summary.py:64\u001b[0m, in \u001b[0;36mModelSummary.on_fit_start\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_depth:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m model_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m summary_data \u001b[38;5;241m=\u001b[39m model_summary\u001b[38;5;241m.\u001b[39m_get_summary_data()\n\u001b[1;32m     66\u001b[0m total_parameters \u001b[38;5;241m=\u001b[39m model_summary\u001b[38;5;241m.\u001b[39mtotal_parameters\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_summary.py:78\u001b[0m, in \u001b[0;36mModelSummary._summary\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mstrategy, DeepSpeedStrategy) \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mzero_stage_3:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DeepSpeedSummary(pl_module, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_depth)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_depth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:475\u001b[0m, in \u001b[0;36msummarize\u001b[0;34m(lightning_module, max_depth)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize\u001b[39m(lightning_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_depth: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelSummary:\n\u001b[1;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Summarize the LightningModule specified by `lightning_module`.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModelSummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:210\u001b[0m, in \u001b[0;36mModelSummary.__init__\u001b[0;34m(self, model, max_depth)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`max_depth` can be -1, 0 or > 0, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_depth \u001b[38;5;241m=\u001b[39m max_depth\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# 1 byte -> 8 bits\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# TODO: how do we compute precision_megabytes in case of mixed precision?\u001b[39;00m\n\u001b[1;32m    213\u001b[0m precision_to_bits \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m64\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m32\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m16\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:271\u001b[0m, in \u001b[0;36mModelSummary.summarize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m summary \u001b[38;5;241m=\u001b[39m OrderedDict((name, LayerSummary(module)) \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_modules)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mexample_input_array \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_example_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    273\u001b[0m     layer\u001b[38;5;241m.\u001b[39mdetach_hook()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:304\u001b[0m, in \u001b[0;36mModelSummary._forward_example_input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m         model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_)\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m mode\u001b[38;5;241m.\u001b[39mrestore(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 53\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Apply Transforrmer\u001b[39;00m\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m---> 53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Global Average Pooling\u001b[39;00m\n\u001b[1;32m     56\u001b[0m pooled \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, D, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, D, H, W) -> (B, D, HW) -> (B, D)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 142\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_net(after_norm_1)\n\u001b[1;32m    141\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_net(after_norm_1)\n\u001b[0;32m--> 142\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspatial_linear_self_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn\n\u001b[1;32m    144\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_2(x)\n",
      "Cell \u001b[0;32mIn[19], line 129\u001b[0m, in \u001b[0;36mTransformerBlock.spatial_linear_self_attention\u001b[0;34m(self, Q, K, V)\u001b[0m\n\u001b[1;32m    121\u001b[0m assert_shape(KV, (B \u001b[38;5;241m*\u001b[39m h, Dq \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m h, Dv \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m h))\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Q (B * num_heads, H, W, Dq//num_heads)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# KV (B * num_heads, Dq//num_heads, Dv//num_heads)\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# functional conv2d\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# weight: KV[i, :, :].unsqueeze(-1).unsqueeze(-1)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# input: Q.permute(0, 3, 1, 2)\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m QKV \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKV\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B * num_heads, HW, Dv//num_heads)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m QKV \u001b[38;5;241m=\u001b[39m rearrange(\n\u001b[1;32m    131\u001b[0m     QKV, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(b head) (h w) d -> b (head d) h w\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39mH, w\u001b[38;5;241m=\u001b[39mW, b\u001b[38;5;241m=\u001b[39mB, head\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, d\u001b[38;5;241m=\u001b[39mD \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads\n\u001b[1;32m    132\u001b[0m )  \u001b[38;5;66;03m# (B, D, H, W)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m QKV \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_unification(QKV)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batch1 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "model, results = train_model(\n",
    "    model_kwargs={\n",
    "        \"embed_dim\": 256,\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 8,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_channels\": 3,\n",
    "        \"num_classes\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    lr=3e-4,\n",
    ")\n",
    "\n",
    "print(\"ViT results\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
